{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08dda8dc-64f3-48c9-855c-cda2d6d6d160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark/python/pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f82b2fa-1bc6-46c9-b86d-0e99176f197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-03-31 13:17:10,867 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-03-31 13:17:17,276 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"yarn\")\n",
    "         .appName(\"python-testing\")\n",
    "         .config(\"spark.executor.instances\", 16)\n",
    "         .config(\"spark.executor.memory\", \"1536m\")\n",
    "         #.config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "         #.config(\"spark.executor.cores\", 1)\n",
    "         #.config(\"spark.dynamicAllocation.minExecutors\", 4)\n",
    "         #.config(\"spark.dynamicAllocation.maxExecutors\", 32)\n",
    "         #.config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "         #.config(\"spark.shuffle.service.port\", 7337)\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afe445b-5204-41d9-8ebd-2d6a5899194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.environment['PYTHONPATH'] = '/home/ubuntu/.local/lib/python3.8/site-packages'\n",
    "sc.addPyFile('hbase_datasketch.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6ddb11-7b16-4de7-8934-bac7d5a7eaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(30126923,\n",
       "  'CGCATCGTGGCTGGACCTGAGTCCATCTGCCCTGGTGCCTGCATGACTGGCCCTTCTCCTTCACAGACCATGGCCCCAGGCTCCCTTGCTTTCATTTCCCAGCCCGTTATTGGGGCAGGAGAGTAGCAAGCGGGGGAGTTTTGATGAGGCGAGGA')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = sc.sequenceFile(\"hdfs:///files/windowed\")\n",
    "windows.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288d82d3-c2ba-45dc-a6d1-48b4ba60dce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = windows.sample(fraction=0.0005, withReplacement=False, seed=1).cache()\n",
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afd8465-e068-439c-b3c4-8224ba15f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasketch as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a45f531-0648-4e32-a53c-7fd6dda28513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashValue(value):\n",
    "    import datasketch as ds\n",
    "    h = ds.MinHash()\n",
    "    for v in value:\n",
    "        h.update(v.encode('utf8'))\n",
    "    return (h.seed, h.hashvalues)\n",
    "\n",
    "hashes = sample.mapValues(hashValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c50d9036-f56f-4e15-9139-8d53c3b55c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh = ds.MinHashLSH(threshold=0.5, num_perm=128, storage_config={\n",
    "        'type': 'redis',\n",
    "        'redis': {'host': 'datanode1', 'port': 6379},\n",
    "        'basename': b'spark_01'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9da44af2-b15b-43d4-b921-91c226c38bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 13:24:05,194 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 4.0 (TID 212) (datanode3 executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000006/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000006/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,201 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 4.0 (TID 207) (datanode2 executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000004/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000004/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,206 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 4.0 (TID 217) (datanode3 executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000010/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000010/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,217 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 4.0 (TID 205) (datanode3 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ExecAbortError: Transaction discarded because of previous errors.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000002/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1910, in _execute_transaction\n",
      "    raise errors[0][1]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1900, in _execute_transaction\n",
      "    self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: Command # 2 (RPUSH spark_01_keys�\u0004�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000Jr�k\u0002. \u0000\u0000\u0000\u0000\u0013��\u0000\u0000\u0000\u0000\u0001H\u001f\u0000\u0000\u0000\u0000)�u5\u0000\u0000\u0000\u0000\u000fW\u0006�\u0000\u0000\u0000\u0000\u0000��- \u0000\u0000\u0000\u0000#t\u0005�\u0000\u0000\u0000\u0000\n",
      "�B\\ \u0000\u0000\u0000\u0000\u0004�\u0000\u0000\u0000\u0000#\u0015��\u0000\u0000\u0000\u0000\u000e\u0015hF\u0000\u0000\u0000\u0000%��v\u0000\u0000\u0000\u00002�y#��\u0000\u0000\u0000\u0000���\t\u0000\u0000\u0000\u00003:��\u0000\u0000\u0000\u0000\u0012+\u0018� \u0000\u0000\u0000\u0000\u001d",
      "_o�\u0000\u0000\u0000\u0000\u0004���\u0000\u0000\u0000\u0000)��\u001f\u0000\u0000\u0000\u0000D!Ȃ\u0000\u0000\u0000\u0000OR/� \u0000\u0000\u0000\u0000V\u0005>`\u0000\u0000\u0000\u00004�\u0010\u0016\u0000\u0000\u0000\u0000\u001c",
      "��(\u0000\u0000\u0000\u0000+�q�\u0000\u0000\u0000\u0000&2EH \u0000\u0000\u0000\u0000tjS�\u0000\u0000\u0000\u0000\u00064�\u0019\u0000\u0000\u0000\u0000\u0014,7�\u0000\u0000\u0000\u0000�8e�\u0000\u0000\u0000\u0000\u0003\u0015oD \u0000\u0000\u0000\u0000\u0018�2\u000e\u0000\u0000\u0000\u0000�[��\u0000\u0000\u0000\u0000s\u0016��\u0000\u0000\u0000\u00007Y-K\u0000\u0000\u0000�ox \u0000\u0000\u0000t��\u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u0000z��}\u0000\u0000\u0000\u0000 \u0012��\u0000\u0000\u0000\u0000\n",
      "y�i\u0000\u0000\u0000\u0000\u001b��;\u0000\u0000\u0000\u0000 �\u0011$ \u0000\u0000\u0000\u0000&��P\u0000\u0000\u0000\u0000\u000b",
      "��b\u0000\u0000\u0000\u0000\u001f+��\u0000\u0000\u0000\u0000\u0005��\u001f\u0000\u0000\u0000\u00009�[� \u0000\u0000\u0000\u0000d��\u0000\u0000\u0000\u0000;\u0004\u0016|\u0000\u0000\u0000\u0000l�ї\u0000\u0000\u0000\u0000\u0013\t\"{\u0000\u0000\u0000\u0000*��7 \u0000\u0000\u0000\u0000$�{^\u0000\u0000\u0000\u0000\u0006\u001c",
      "�\u000f\u0000\u0000\u0000\u0000S�q�\u0000\u0000\u0000\u0000��'�\u0000\u0000\u0000\u0000���Z \u0000\u0000\u0000\u00008�iR\u0000\u0000\u0000\u0000\u0006̕�\u0000\u0000\u0000\u0000\u0001�o�\u0000\u0000\u0000\u0000'\u00058|\u0000\u0000\u0000\u0000<:�\u0018 \u0000\u0000\u0000\u0000kT�\u0016\u0000\u0000\u0000\u0000*�;\u0000\u0000\u0000\u00008�u�\u0000\u0000\u0000\u0000\u0003��,\u0000\u0000\u0000\u0000:�P� \u0000\u0000\u0000\u001e",
      "(\u001f\u0000\u0000\u0000\u0000\u0000��F\u0000\u0000\u0000\u0000i\\\u0017\t\u0000\u0000\u0000\u0000\u0018�E�\u0000\u0000\u0000\u0000�[4^ \u0000\u0000\u0000\u0000<\u001fݫ\u0000\u0000\u0000\u0000WJ>\u0000\u0000\u0000\u0000\u0000���+\u0000\u0000\u0000\u0000\t\u0004\u0003�\u0000\u0000\u0000\u0000�-:� \u0000\u0000\u0000\u0000!�*�\u0000\u0000\u0000\u0000k+�\\\u0000\u0000\u0000\u0000\u0001�\u001d",
      "\u001d",
      "\u0000\u0000\u0000\u0000~\f",
      "��\u0000\u0000\u0000\u0000\u0005Ч� \u0000\u0000\u0000\u0000I��\u0004\u0000\u0000\u0000\u0000\u0003��\u0005\u0000\u0000\u0000\u0000`�\u0014p\u0000\u0000\u0000\u0000\u0010��Y\u0000\u0000\u0000\u0000Pc� \u0000\u0000\u0000\u0000\u0000װH\u0000\u0000\u0000\u0000\u001e",
      "�y{\u0000\u0000\u0000\u0000\u001b\tu`\u0000\u0000\u0000\u0000\u0010s#�\u0000\u0000\u0000\u0000B\u000e�� \u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u00000�P_\u0000\u0000\u0000\u0000\"�l\\\u0000\u0000\u0000\u0000>[B�\u0000\u0000\u0000\u0000cO�6 \u0000\u0000\u0000\u0000M9��\u0000\u0000\u0000\u0000\u0004��`\u0000\u0000\u0000\u0000\u000e�w�\u0000\u0000\u0000\u00007�>�\u0000\u0000\u0000\u0000\u0012\u0001�{) of pipeline caused error: MOVED 2251 10.10.1.150:6379\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,233 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 4.0 (TID 218) (datanode1 executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000011/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000011/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,241 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 4.0 (TID 208) (datanode1 executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000015/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000015/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,248 WARN scheduler.TaskSetManager: Lost task 20.0 in stage 4.0 (TID 206) (datanode2 executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000016/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000016/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,254 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 4.0 (TID 211) (datanode1 executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000007/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000007/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,260 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 4.0 (TID 214) (datanode1 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,268 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 4.0 (TID 215) (datanode2 executor 11): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000012/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000012/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,281 WARN scheduler.TaskSetManager: Lost task 14.0 in stage 4.0 (TID 210) (datanode2 executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000008/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000008/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,288 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 4.0 (TID 216) (datanode4 executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ExecAbortError: Transaction discarded because of previous errors.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000009/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000009/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1910, in _execute_transaction\n",
      "    raise errors[0][1]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1900, in _execute_transaction\n",
      "    self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: Command # 2 (RPUSH spark_01_keys�\u0004�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000J��c\u0000. \u0000\u0000\u0000\u0000\u0013��\u0000\u0000\u0000\u0000\u0001H\u001f\u0000\u0000\u0000\u0000)�u5\u0000\u0000\u0000\u0000\u000fW\u0006�\u0000\u0000\u0000\u0000\u0000��- \u0000\u0000\u0000\u0000#t\u0005�\u0000\u0000\u0000\u0000\n",
      "�B\\ \u0000\u0000\u0000\u0000\u0004�\u0000\u0000\u0000\u0000#\u0015��\u0000\u0000\u0000\u0000\u000e\u0015hF\u0000\u0000\u0000\u0000%��v\u0000\u0000\u0000\u00002�y#��\u0000\u0000\u0000\u0000���\t\u0000\u0000\u0000\u00003:��\u0000\u0000\u0000\u0000\u0012+\u0018� \u0000\u0000\u0000\u0000\u001d",
      "_o�\u0000\u0000\u0000\u0000\u0004���\u0000\u0000\u0000\u0000)��\u001f\u0000\u0000\u0000\u0000D!Ȃ\u0000\u0000\u0000\u0000OR/� \u0000\u0000\u0000\u0000V\u0005>`\u0000\u0000\u0000\u00004�\u0010\u0016\u0000\u0000\u0000\u0000\u001c",
      "��(\u0000\u0000\u0000\u0000+�q�\u0000\u0000\u0000\u0000&2EH \u0000\u0000\u0000\u0000tjS�\u0000\u0000\u0000\u0000\u00064�\u0019\u0000\u0000\u0000\u0000\u0014,7�\u0000\u0000\u0000\u0000�8e�\u0000\u0000\u0000\u0000\u0003\u0015oD \u0000\u0000\u0000\u0000\u0018�2\u000e\u0000\u0000\u0000\u0000�[��\u0000\u0000\u0000\u0000s\u0016��\u0000\u0000\u0000\u00007Y-K\u0000\u0000\u0000�ox \u0000\u0000\u0000t��\u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u0000z��}\u0000\u0000\u0000\u0000 \u0012��\u0000\u0000\u0000\u0000\n",
      "y�i\u0000\u0000\u0000\u0000\u001b��;\u0000\u0000\u0000\u0000 �\u0011$ \u0000\u0000\u0000\u0000&��P\u0000\u0000\u0000\u0000\u000b",
      "��b\u0000\u0000\u0000\u0000\u001f+��\u0000\u0000\u0000\u0000\u0005��\u001f\u0000\u0000\u0000\u00009�[� \u0000\u0000\u0000\u0000d��\u0000\u0000\u0000\u0000;\u0004\u0016|\u0000\u0000\u0000\u0000l�ї\u0000\u0000\u0000\u0000\u0013\t\"{\u0000\u0000\u0000\u0000*��7 \u0000\u0000\u0000\u0000$�{^\u0000\u0000\u0000\u0000\u0006\u001c",
      "�\u000f\u0000\u0000\u0000\u0000S�q�\u0000\u0000\u0000\u0000��'�\u0000\u0000\u0000\u0000���Z \u0000\u0000\u0000\u00008�iR\u0000\u0000\u0000\u0000\u0006̕�\u0000\u0000\u0000\u0000\u0001�o�\u0000\u0000\u0000\u0000'\u00058|\u0000\u0000\u0000\u0000<:�\u0018 \u0000\u0000\u0000\u0000kT�\u0016\u0000\u0000\u0000\u0000*�;\u0000\u0000\u0000\u00008�u�\u0000\u0000\u0000\u0000\u0003��,\u0000\u0000\u0000\u0000:�P� \u0000\u0000\u0000\u001e",
      "(\u001f\u0000\u0000\u0000\u0000\u0000��F\u0000\u0000\u0000\u0000i\\\u0017\t\u0000\u0000\u0000\u0000\u0018�E�\u0000\u0000\u0000\u0000�[4^ \u0000\u0000\u0000\u0000<\u001fݫ\u0000\u0000\u0000\u0000WJ>\u0000\u0000\u0000\u0000\u0000���+\u0000\u0000\u0000\u0000\t\u0004\u0003�\u0000\u0000\u0000\u0000�-:� \u0000\u0000\u0000\u0000!�*�\u0000\u0000\u0000\u0000k+�\\\u0000\u0000\u0000\u0000\u0001�\u001d",
      "\u001d",
      "\u0000\u0000\u0000\u0000~\f",
      "��\u0000\u0000\u0000\u0000\u0005Ч� \u0000\u0000\u0000\u0000I��\u0004\u0000\u0000\u0000\u0000\u0003��\u0005\u0000\u0000\u0000\u0000`�\u0014p\u0000\u0000\u0000\u0000\u0010��Y\u0000\u0000\u0000\u0000Pc� \u0000\u0000\u0000\u0000\u0000װH\u0000\u0000\u0000\u0000\u001e",
      "�y{\u0000\u0000\u0000\u0000\u001b\tu`\u0000\u0000\u0000\u0000\u0010s#�\u0000\u0000\u0000\u0000B\u000e�� \u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u00000�P_\u0000\u0000\u0000\u0000\"�l\\\u0000\u0000\u0000\u0000>[B�\u0000\u0000\u0000\u0000cO�6 \u0000\u0000\u0000\u0000M9��\u0000\u0000\u0000\u0000\u0004��`\u0000\u0000\u0000\u0000\u000e�w�\u0000\u0000\u0000\u00007�>�\u0000\u0000\u0000\u0000\u0012\u0001�{) of pipeline caused error: MOVED 2958 10.10.1.150:6379\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,296 WARN scheduler.TaskSetManager: Lost task 12.0 in stage 4.0 (TID 209) (datanode4 executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ExecAbortError: Transaction discarded because of previous errors.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1910, in _execute_transaction\n",
      "    raise errors[0][1]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1900, in _execute_transaction\n",
      "    self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: Command # 2 (RPUSH spark_01_keys�\u0004�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000J��}\u0002. \u0000\u0000\u0000\u0000\u0013��\u0000\u0000\u0000\u0000\u0001H\u001f\u0000\u0000\u0000\u0000)�u5\u0000\u0000\u0000\u0000\u000fW\u0006�\u0000\u0000\u0000\u0000\u0000��- \u0000\u0000\u0000\u0000#t\u0005�\u0000\u0000\u0000\u0000\n",
      "�B\\ \u0000\u0000\u0000\u0000\u0004�\u0000\u0000\u0000\u0000#\u0015��\u0000\u0000\u0000\u0000\u000e\u0015hF\u0000\u0000\u0000\u0000%��v\u0000\u0000\u0000\u00002�y#��\u0000\u0000\u0000\u0000���\t\u0000\u0000\u0000\u00003:��\u0000\u0000\u0000\u0000\u0012+\u0018� \u0000\u0000\u0000\u0000\u001d",
      "_o�\u0000\u0000\u0000\u0000\u0004���\u0000\u0000\u0000\u0000)��\u001f\u0000\u0000\u0000\u0000D!Ȃ\u0000\u0000\u0000\u0000OR/� \u0000\u0000\u0000\u0000V\u0005>`\u0000\u0000\u0000\u00004�\u0010\u0016\u0000\u0000\u0000\u0000\u001c",
      "��(\u0000\u0000\u0000\u0000+�q�\u0000\u0000\u0000\u0000&2EH \u0000\u0000\u0000\u0000tjS�\u0000\u0000\u0000\u0000\u00064�\u0019\u0000\u0000\u0000\u0000\u0014,7�\u0000\u0000\u0000\u0000�8e�\u0000\u0000\u0000\u0000\u0003\u0015oD \u0000\u0000\u0000\u0000\u0018�2\u000e\u0000\u0000\u0000\u0000�[��\u0000\u0000\u0000\u0000s\u0016��\u0000\u0000\u0000\u00007Y-K\u0000\u0000\u0000�ox \u0000\u0000\u0000t��\u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u0000z��}\u0000\u0000\u0000\u0000 \u0012��\u0000\u0000\u0000\u0000\n",
      "y�i\u0000\u0000\u0000\u0000\u001b��;\u0000\u0000\u0000\u0000 �\u0011$ \u0000\u0000\u0000\u0000&��P\u0000\u0000\u0000\u0000\u000b",
      "��b\u0000\u0000\u0000\u0000\u001f+��\u0000\u0000\u0000\u0000\u0005��\u001f\u0000\u0000\u0000\u00009�[� \u0000\u0000\u0000\u0000d��\u0000\u0000\u0000\u0000;\u0004\u0016|\u0000\u0000\u0000\u0000l�ї\u0000\u0000\u0000\u0000\u0013\t\"{\u0000\u0000\u0000\u0000*��7 \u0000\u0000\u0000\u0000$�{^\u0000\u0000\u0000\u0000\u0006\u001c",
      "�\u000f\u0000\u0000\u0000\u0000S�q�\u0000\u0000\u0000\u0000��'�\u0000\u0000\u0000\u0000���Z \u0000\u0000\u0000\u00008�iR\u0000\u0000\u0000\u0000\u0006̕�\u0000\u0000\u0000\u0000\u0001�o�\u0000\u0000\u0000\u0000'\u00058|\u0000\u0000\u0000\u0000<:�\u0018 \u0000\u0000\u0000\u0000kT�\u0016\u0000\u0000\u0000\u0000*�;\u0000\u0000\u0000\u00008�u�\u0000\u0000\u0000\u0000\u0003��,\u0000\u0000\u0000\u0000:�P� \u0000\u0000\u0000\u001e",
      "(\u001f\u0000\u0000\u0000\u0000\u0000��F\u0000\u0000\u0000\u0000i\\\u0017\t\u0000\u0000\u0000\u0000\u0018�E�\u0000\u0000\u0000\u0000�[4^ \u0000\u0000\u0000\u0000<\u001fݫ\u0000\u0000\u0000\u0000WJ>\u0000\u0000\u0000\u0000\u0000���+\u0000\u0000\u0000\u0000\t\u0004\u0003�\u0000\u0000\u0000\u0000�-:� \u0000\u0000\u0000\u0000!�*�\u0000\u0000\u0000\u0000k+�\\\u0000\u0000\u0000\u0000\u0001�\u001d",
      "\u001d",
      "\u0000\u0000\u0000\u0000~\f",
      "��\u0000\u0000\u0000\u0000\u0005Ч� \u0000\u0000\u0000\u0000I��\u0004\u0000\u0000\u0000\u0000\u0003��\u0005\u0000\u0000\u0000\u0000`�\u0014p\u0000\u0000\u0000\u0000\u0010��Y\u0000\u0000\u0000\u0000Pc� \u0000\u0000\u0000\u0000\u0000װH\u0000\u0000\u0000\u0000\u001e",
      "�y{\u0000\u0000\u0000\u0000\u001b\tu`\u0000\u0000\u0000\u0000\u0010s#�\u0000\u0000\u0000\u0000B\u000e�� \u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u00000�P_\u0000\u0000\u0000\u0000\"�l\\\u0000\u0000\u0000\u0000>[B�\u0000\u0000\u0000\u0000cO�6 \u0000\u0000\u0000\u0000M9��\u0000\u0000\u0000\u0000\u0004��`\u0000\u0000\u0000\u0000\u000e�w�\u0000\u0000\u0000\u00007�>�\u0000\u0000\u0000\u0000\u0012\u0001�{) of pipeline caused error: MOVED 7833 10.10.1.27:6379\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,307 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 4.0 (TID 204) (datanode4 executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ExecAbortError: Transaction discarded because of previous errors.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000005/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000005/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1910, in _execute_transaction\n",
      "    raise errors[0][1]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1900, in _execute_transaction\n",
      "    self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: Command # 2 (RPUSH spark_01_keys�\u0004�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000J��X\u0003. \u0000\u0000\u0000\u0000\u0013��\u0000\u0000\u0000\u0000\u0001H\u001f\u0000\u0000\u0000\u0000)�u5\u0000\u0000\u0000\u0000\u000fW\u0006�\u0000\u0000\u0000\u0000\u0000��- \u0000\u0000\u0000\u0000#t\u0005�\u0000\u0000\u0000\u0000\n",
      "�B\\ \u0000\u0000\u0000\u0000\u0004�\u0000\u0000\u0000\u0000#\u0015��\u0000\u0000\u0000\u0000\u000e\u0015hF\u0000\u0000\u0000\u0000%��v\u0000\u0000\u0000\u00002�y#��\u0000\u0000\u0000\u0000���\t\u0000\u0000\u0000\u00003:��\u0000\u0000\u0000\u0000\u0012+\u0018� \u0000\u0000\u0000\u0000\u001d",
      "_o�\u0000\u0000\u0000\u0000\u0004���\u0000\u0000\u0000\u0000)��\u001f\u0000\u0000\u0000\u0000D!Ȃ\u0000\u0000\u0000\u0000OR/� \u0000\u0000\u0000\u0000V\u0005>`\u0000\u0000\u0000\u00004�\u0010\u0016\u0000\u0000\u0000\u0000\u001c",
      "��(\u0000\u0000\u0000\u0000+�q�\u0000\u0000\u0000\u0000&2EH \u0000\u0000\u0000\u0000tjS�\u0000\u0000\u0000\u0000\u00064�\u0019\u0000\u0000\u0000\u0000\u0014,7�\u0000\u0000\u0000\u0000�8e�\u0000\u0000\u0000\u0000\u0003\u0015oD \u0000\u0000\u0000\u0000\u0018�2\u000e\u0000\u0000\u0000\u0000�[��\u0000\u0000\u0000\u0000s\u0016��\u0000\u0000\u0000\u00007Y-K\u0000\u0000\u0000�ox \u0000\u0000\u0000t��\u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u0000z��}\u0000\u0000\u0000\u0000 \u0012��\u0000\u0000\u0000\u0000\n",
      "y�i\u0000\u0000\u0000\u0000\u001b��;\u0000\u0000\u0000\u0000 �\u0011$ \u0000\u0000\u0000\u0000&��P\u0000\u0000\u0000\u0000\u000b",
      "��b\u0000\u0000\u0000\u0000\u001f+��\u0000\u0000\u0000\u0000\u0005��\u001f\u0000\u0000\u0000\u00009�[� \u0000\u0000\u0000\u0000d��\u0000\u0000\u0000\u0000;\u0004\u0016|\u0000\u0000\u0000\u0000l�ї\u0000\u0000\u0000\u0000\u0013\t\"{\u0000\u0000\u0000\u0000*��7 \u0000\u0000\u0000\u0000$�{^\u0000\u0000\u0000\u0000\u0006\u001c",
      "�\u000f\u0000\u0000\u0000\u0000S�q�\u0000\u0000\u0000\u0000��'�\u0000\u0000\u0000\u0000���Z \u0000\u0000\u0000\u00008�iR\u0000\u0000\u0000\u0000\u0006̕�\u0000\u0000\u0000\u0000\u0001�o�\u0000\u0000\u0000\u0000'\u00058|\u0000\u0000\u0000\u0000<:�\u0018 \u0000\u0000\u0000\u0000kT�\u0016\u0000\u0000\u0000\u0000*�;\u0000\u0000\u0000\u00008�u�\u0000\u0000\u0000\u0000\u0003��,\u0000\u0000\u0000\u0000:�P� \u0000\u0000\u0000\u001e",
      "(\u001f\u0000\u0000\u0000\u0000\u0000��F\u0000\u0000\u0000\u0000i\\\u0017\t\u0000\u0000\u0000\u0000\u0018�E�\u0000\u0000\u0000\u0000�[4^ \u0000\u0000\u0000\u0000<\u001fݫ\u0000\u0000\u0000\u0000WJ>\u0000\u0000\u0000\u0000\u0000���+\u0000\u0000\u0000\u0000\t\u0004\u0003�\u0000\u0000\u0000\u0000�-:� \u0000\u0000\u0000\u0000!�*�\u0000\u0000\u0000\u0000k+�\\\u0000\u0000\u0000\u0000\u0001�\u001d",
      "\u001d",
      "\u0000\u0000\u0000\u0000~\f",
      "��\u0000\u0000\u0000\u0000\u0005Ч� \u0000\u0000\u0000\u0000I��\u0004\u0000\u0000\u0000\u0000\u0003��\u0005\u0000\u0000\u0000\u0000`�\u0014p\u0000\u0000\u0000\u0000\u0010��Y\u0000\u0000\u0000\u0000Pc� \u0000\u0000\u0000\u0000\u0000װH\u0000\u0000\u0000\u0000\u001e",
      "�y{\u0000\u0000\u0000\u0000\u001b\tu`\u0000\u0000\u0000\u0000\u0010s#�\u0000\u0000\u0000\u0000B\u000e�� \u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u00000�P_\u0000\u0000\u0000\u0000\"�l\\\u0000\u0000\u0000\u0000>[B�\u0000\u0000\u0000\u0000cO�6 \u0000\u0000\u0000\u0000M9��\u0000\u0000\u0000\u0000\u0004��`\u0000\u0000\u0000\u0000\u000e�w�\u0000\u0000\u0000\u00007�>�\u0000\u0000\u0000\u0000\u0012\u0001�{) of pipeline caused error: MOVED 4697 10.10.1.27:6379\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:05,315 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 4.0 (TID 213) (datanode4 executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ExecAbortError: Transaction discarded because of previous errors.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000013/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000013/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1910, in _execute_transaction\n",
      "    raise errors[0][1]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1900, in _execute_transaction\n",
      "    self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: Command # 2 (RPUSH spark_01_keys�\u0004�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000J \u0006}\u0000. \u0000\u0000\u0000\u0000\u0013��\u0000\u0000\u0000\u0000\u0001H\u001f\u0000\u0000\u0000\u0000)�u5\u0000\u0000\u0000\u0000\u000fW\u0006�\u0000\u0000\u0000\u0000\u0000��- \u0000\u0000\u0000\u0000#t\u0005�\u0000\u0000\u0000\u0000\n",
      "�B\\ \u0000\u0000\u0000\u0000\u0004�\u0000\u0000\u0000\u0000#\u0015��\u0000\u0000\u0000\u0000\u000e\u0015hF\u0000\u0000\u0000\u0000%��v\u0000\u0000\u0000\u00002�y#��\u0000\u0000\u0000\u0000���\t\u0000\u0000\u0000\u00003:��\u0000\u0000\u0000\u0000\u0012+\u0018� \u0000\u0000\u0000\u0000\u001d",
      "_o�\u0000\u0000\u0000\u0000\u0004���\u0000\u0000\u0000\u0000)��\u001f\u0000\u0000\u0000\u0000D!Ȃ\u0000\u0000\u0000\u0000OR/� \u0000\u0000\u0000\u0000V\u0005>`\u0000\u0000\u0000\u00004�\u0010\u0016\u0000\u0000\u0000\u0000\u001c",
      "��(\u0000\u0000\u0000\u0000+�q�\u0000\u0000\u0000\u0000&2EH \u0000\u0000\u0000\u0000tjS�\u0000\u0000\u0000\u0000\u00064�\u0019\u0000\u0000\u0000\u0000\u0014,7�\u0000\u0000\u0000\u0000�8e�\u0000\u0000\u0000\u0000\u0003\u0015oD \u0000\u0000\u0000\u0000\u0018�2\u000e\u0000\u0000\u0000\u0000�[��\u0000\u0000\u0000\u0000s\u0016��\u0000\u0000\u0000\u00007Y-K\u0000\u0000\u0000�ox \u0000\u0000\u0000t��\u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u0000z��}\u0000\u0000\u0000\u0000 \u0012��\u0000\u0000\u0000\u0000\n",
      "y�i\u0000\u0000\u0000\u0000\u001b��;\u0000\u0000\u0000\u0000 �\u0011$ \u0000\u0000\u0000\u0000&��P\u0000\u0000\u0000\u0000\u000b",
      "��b\u0000\u0000\u0000\u0000\u001f+��\u0000\u0000\u0000\u0000\u0005��\u001f\u0000\u0000\u0000\u00009�[� \u0000\u0000\u0000\u0000d��\u0000\u0000\u0000\u0000;\u0004\u0016|\u0000\u0000\u0000\u0000l�ї\u0000\u0000\u0000\u0000\u0013\t\"{\u0000\u0000\u0000\u0000*��7 \u0000\u0000\u0000\u0000$�{^\u0000\u0000\u0000\u0000\u0006\u001c",
      "�\u000f\u0000\u0000\u0000\u0000S�q�\u0000\u0000\u0000\u0000��'�\u0000\u0000\u0000\u0000���Z \u0000\u0000\u0000\u00008�iR\u0000\u0000\u0000\u0000\u0006̕�\u0000\u0000\u0000\u0000\u0001�o�\u0000\u0000\u0000\u0000'\u00058|\u0000\u0000\u0000\u0000<:�\u0018 \u0000\u0000\u0000\u0000kT�\u0016\u0000\u0000\u0000\u0000*�;\u0000\u0000\u0000\u00008�u�\u0000\u0000\u0000\u0000\u0003��,\u0000\u0000\u0000\u0000:�P� \u0000\u0000\u0000\u001e",
      "(\u001f\u0000\u0000\u0000\u0000\u0000��F\u0000\u0000\u0000\u0000i\\\u0017\t\u0000\u0000\u0000\u0000\u0018�E�\u0000\u0000\u0000\u0000�[4^ \u0000\u0000\u0000\u0000<\u001fݫ\u0000\u0000\u0000\u0000WJ>\u0000\u0000\u0000\u0000\u0000���+\u0000\u0000\u0000\u0000\t\u0004\u0003�\u0000\u0000\u0000\u0000�-:� \u0000\u0000\u0000\u0000!�*�\u0000\u0000\u0000\u0000k+�\\\u0000\u0000\u0000\u0000\u0001�\u001d",
      "\u001d",
      "\u0000\u0000\u0000\u0000~\f",
      "��\u0000\u0000\u0000\u0000\u0005Ч� \u0000\u0000\u0000\u0000I��\u0004\u0000\u0000\u0000\u0000\u0003��\u0005\u0000\u0000\u0000\u0000`�\u0014p\u0000\u0000\u0000\u0000\u0010��Y\u0000\u0000\u0000\u0000Pc� \u0000\u0000\u0000\u0000\u0000װH\u0000\u0000\u0000\u0000\u001e",
      "�y{\u0000\u0000\u0000\u0000\u001b\tu`\u0000\u0000\u0000\u0000\u0010s#�\u0000\u0000\u0000\u0000B\u000e�� \u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u00000�P_\u0000\u0000\u0000\u0000\"�l\\\u0000\u0000\u0000\u0000>[B�\u0000\u0000\u0000\u0000cO�6 \u0000\u0000\u0000\u0000M9��\u0000\u0000\u0000\u0000\u0004��`\u0000\u0000\u0000\u0000\u000e�w�\u0000\u0000\u0000\u00007�>�\u0000\u0000\u0000\u0000\u0012\u0001�{) of pipeline caused error: MOVED 2111 10.10.1.150:6379\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:06,272 WARN scheduler.TaskSetManager: Lost task 30.0 in stage 4.0 (TID 237) (datanode1 executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ExecAbortError: Transaction discarded because of previous errors.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000007/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000007/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1910, in _execute_transaction\n",
      "    raise errors[0][1]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1900, in _execute_transaction\n",
      "    self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: Command # 2 (RPUSH spark_01_keys�\u0004�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000J)Z�\u0002. \u0000\u0000\u0000\u0000\u0013��\u0000\u0000\u0000\u0000\u0001H\u001f\u0000\u0000\u0000\u0000)�u5\u0000\u0000\u0000\u0000\u000fW\u0006�\u0000\u0000\u0000\u0000\u0000��- \u0000\u0000\u0000\u0000#t\u0005�\u0000\u0000\u0000\u0000\n",
      "�B\\ \u0000\u0000\u0000\u0000\u0004�\u0000\u0000\u0000\u0000#\u0015��\u0000\u0000\u0000\u0000\u000e\u0015hF\u0000\u0000\u0000\u0000%��v\u0000\u0000\u0000\u00002�y#��\u0000\u0000\u0000\u0000���\t\u0000\u0000\u0000\u00003:��\u0000\u0000\u0000\u0000\u0012+\u0018� \u0000\u0000\u0000\u0000\u001d",
      "_o�\u0000\u0000\u0000\u0000\u0004���\u0000\u0000\u0000\u0000)��\u001f\u0000\u0000\u0000\u0000D!Ȃ\u0000\u0000\u0000\u0000OR/� \u0000\u0000\u0000\u0000V\u0005>`\u0000\u0000\u0000\u00004�\u0010\u0016\u0000\u0000\u0000\u0000\u001c",
      "��(\u0000\u0000\u0000\u0000+�q�\u0000\u0000\u0000\u0000&2EH \u0000\u0000\u0000\u0000tjS�\u0000\u0000\u0000\u0000\u00064�\u0019\u0000\u0000\u0000\u0000\u0014,7�\u0000\u0000\u0000\u0000�8e�\u0000\u0000\u0000\u0000\u0003\u0015oD \u0000\u0000\u0000\u0000\u0018�2\u000e\u0000\u0000\u0000\u0000�[��\u0000\u0000\u0000\u0000s\u0016��\u0000\u0000\u0000\u00007Y-K\u0000\u0000\u0000�ox \u0000\u0000\u0000t��\u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u0000z��}\u0000\u0000\u0000\u0000 \u0012��\u0000\u0000\u0000\u0000\n",
      "y�i\u0000\u0000\u0000\u0000\u001b��;\u0000\u0000\u0000\u0000 �\u0011$ \u0000\u0000\u0000\u0000&��P\u0000\u0000\u0000\u0000\u000b",
      "��b\u0000\u0000\u0000\u0000\u001f+��\u0000\u0000\u0000\u0000\u0005��\u001f\u0000\u0000\u0000\u00009�[� \u0000\u0000\u0000\u0000d��\u0000\u0000\u0000\u0000;\u0004\u0016|\u0000\u0000\u0000\u0000l�ї\u0000\u0000\u0000\u0000\u0013\t\"{\u0000\u0000\u0000\u0000*��7 \u0000\u0000\u0000\u0000$�{^\u0000\u0000\u0000\u0000\u0006\u001c",
      "�\u000f\u0000\u0000\u0000\u0000S�q�\u0000\u0000\u0000\u0000��'�\u0000\u0000\u0000\u0000���Z \u0000\u0000\u0000\u00008�iR\u0000\u0000\u0000\u0000\u0006̕�\u0000\u0000\u0000\u0000\u0001�o�\u0000\u0000\u0000\u0000'\u00058|\u0000\u0000\u0000\u0000<:�\u0018 \u0000\u0000\u0000\u0000kT�\u0016\u0000\u0000\u0000\u0000*�;\u0000\u0000\u0000\u00008�u�\u0000\u0000\u0000\u0000\u0003��,\u0000\u0000\u0000\u0000:�P� \u0000\u0000\u0000\u001e",
      "(\u001f\u0000\u0000\u0000\u0000\u0000��F\u0000\u0000\u0000\u0000i\\\u0017\t\u0000\u0000\u0000\u0000\u0018�E�\u0000\u0000\u0000\u0000�[4^ \u0000\u0000\u0000\u0000<\u001fݫ\u0000\u0000\u0000\u0000WJ>\u0000\u0000\u0000\u0000\u0000���+\u0000\u0000\u0000\u0000\t\u0004\u0003�\u0000\u0000\u0000\u0000�-:� \u0000\u0000\u0000\u0000!�*�\u0000\u0000\u0000\u0000k+�\\\u0000\u0000\u0000\u0000\u0001�\u001d",
      "\u001d",
      "\u0000\u0000\u0000\u0000~\f",
      "��\u0000\u0000\u0000\u0000\u0005Ч� \u0000\u0000\u0000\u0000I��\u0004\u0000\u0000\u0000\u0000\u0003��\u0005\u0000\u0000\u0000\u0000`�\u0014p\u0000\u0000\u0000\u0000\u0010��Y\u0000\u0000\u0000\u0000Pc� \u0000\u0000\u0000\u0000\u0000װH\u0000\u0000\u0000\u0000\u001e",
      "�y{\u0000\u0000\u0000\u0000\u001b\tu`\u0000\u0000\u0000\u0000\u0010s#�\u0000\u0000\u0000\u0000B\u000e�� \u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u00000�P_\u0000\u0000\u0000\u0000\"�l\\\u0000\u0000\u0000\u0000>[B�\u0000\u0000\u0000\u0000cO�6 \u0000\u0000\u0000\u0000M9��\u0000\u0000\u0000\u0000\u0004��`\u0000\u0000\u0000\u0000\u000e�w�\u0000\u0000\u0000\u00007�>�\u0000\u0000\u0000\u0000\u0012\u0001�{) of pipeline caused error: MOVED 1567 10.10.1.150:6379\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:06,288 WARN scheduler.TaskSetManager: Lost task 26.0 in stage 4.0 (TID 239) (datanode1 executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ExecAbortError: Transaction discarded because of previous errors.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000011/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000011/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1910, in _execute_transaction\n",
      "    raise errors[0][1]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1900, in _execute_transaction\n",
      "    self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: Command # 2 (RPUSH spark_01_keys�\u0004�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000J�q�\u0002. \u0000\u0000\u0000\u0000\u0013��\u0000\u0000\u0000\u0000\u0001H\u001f\u0000\u0000\u0000\u0000)�u5\u0000\u0000\u0000\u0000\u000fW\u0006�\u0000\u0000\u0000\u0000\u0000��- \u0000\u0000\u0000\u0000#t\u0005�\u0000\u0000\u0000\u0000\n",
      "�B\\ \u0000\u0000\u0000\u0000\u0004�\u0000\u0000\u0000\u0000#\u0015��\u0000\u0000\u0000\u0000\u000e\u0015hF\u0000\u0000\u0000\u0000%��v\u0000\u0000\u0000\u00002�y#��\u0000\u0000\u0000\u0000���\t\u0000\u0000\u0000\u00003:��\u0000\u0000\u0000\u0000\u0012+\u0018� \u0000\u0000\u0000\u0000\u001d",
      "_o�\u0000\u0000\u0000\u0000\u0004���\u0000\u0000\u0000\u0000)��\u001f\u0000\u0000\u0000\u0000D!Ȃ\u0000\u0000\u0000\u0000OR/� \u0000\u0000\u0000\u0000V\u0005>`\u0000\u0000\u0000\u00004�\u0010\u0016\u0000\u0000\u0000\u0000\u001c",
      "��(\u0000\u0000\u0000\u0000+�q�\u0000\u0000\u0000\u0000&2EH \u0000\u0000\u0000\u0000tjS�\u0000\u0000\u0000\u0000\u00064�\u0019\u0000\u0000\u0000\u0000\u0014,7�\u0000\u0000\u0000\u0000�8e�\u0000\u0000\u0000\u0000\u0003\u0015oD \u0000\u0000\u0000\u0000\u0018�2\u000e\u0000\u0000\u0000\u0000�[��\u0000\u0000\u0000\u0000s\u0016��\u0000\u0000\u0000\u00007Y-K\u0000\u0000\u0000�ox \u0000\u0000\u0000t��\u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u0000z��}\u0000\u0000\u0000\u0000 \u0012��\u0000\u0000\u0000\u0000\n",
      "y�i\u0000\u0000\u0000\u0000\u001b��;\u0000\u0000\u0000\u0000 �\u0011$ \u0000\u0000\u0000\u0000&��P\u0000\u0000\u0000\u0000\u000b",
      "��b\u0000\u0000\u0000\u0000\u001f+��\u0000\u0000\u0000\u0000\u0005��\u001f\u0000\u0000\u0000\u00009�[� \u0000\u0000\u0000\u0000d��\u0000\u0000\u0000\u0000;\u0004\u0016|\u0000\u0000\u0000\u0000l�ї\u0000\u0000\u0000\u0000\u0013\t\"{\u0000\u0000\u0000\u0000*��7 \u0000\u0000\u0000\u0000$�{^\u0000\u0000\u0000\u0000\u0006\u001c",
      "�\u000f\u0000\u0000\u0000\u0000S�q�\u0000\u0000\u0000\u0000��'�\u0000\u0000\u0000\u0000���Z \u0000\u0000\u0000\u00008�iR\u0000\u0000\u0000\u0000\u0006̕�\u0000\u0000\u0000\u0000\u0001�o�\u0000\u0000\u0000\u0000'\u00058|\u0000\u0000\u0000\u0000<:�\u0018 \u0000\u0000\u0000\u0000kT�\u0016\u0000\u0000\u0000\u0000*�;\u0000\u0000\u0000\u00008�u�\u0000\u0000\u0000\u0000\u0003��,\u0000\u0000\u0000\u0000:�P� \u0000\u0000\u0000\u001e",
      "(\u001f\u0000\u0000\u0000\u0000\u0000��F\u0000\u0000\u0000\u0000i\\\u0017\t\u0000\u0000\u0000\u0000\u0018�E�\u0000\u0000\u0000\u0000�[4^ \u0000\u0000\u0000\u0000<\u001fݫ\u0000\u0000\u0000\u0000WJ>\u0000\u0000\u0000\u0000\u0000���+\u0000\u0000\u0000\u0000\t\u0004\u0003�\u0000\u0000\u0000\u0000�-:� \u0000\u0000\u0000\u0000!�*�\u0000\u0000\u0000\u0000k+�\\\u0000\u0000\u0000\u0000\u0001�\u001d",
      "\u001d",
      "\u0000\u0000\u0000\u0000~\f",
      "��\u0000\u0000\u0000\u0000\u0005Ч� \u0000\u0000\u0000\u0000I��\u0004\u0000\u0000\u0000\u0000\u0003��\u0005\u0000\u0000\u0000\u0000`�\u0014p\u0000\u0000\u0000\u0000\u0010��Y\u0000\u0000\u0000\u0000Pc� \u0000\u0000\u0000\u0000\u0000װH\u0000\u0000\u0000\u0000\u001e",
      "�y{\u0000\u0000\u0000\u0000\u001b\tu`\u0000\u0000\u0000\u0000\u0010s#�\u0000\u0000\u0000\u0000B\u000e�� \u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u00000�P_\u0000\u0000\u0000\u0000\"�l\\\u0000\u0000\u0000\u0000>[B�\u0000\u0000\u0000\u0000cO�6 \u0000\u0000\u0000\u0000M9��\u0000\u0000\u0000\u0000\u0004��`\u0000\u0000\u0000\u0000\u000e�w�\u0000\u0000\u0000\u00007�>�\u0000\u0000\u0000\u0000\u0012\u0001�{) of pipeline caused error: MOVED 2145 10.10.1.150:6379\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:06,316 WARN scheduler.TaskSetManager: Lost task 34.0 in stage 4.0 (TID 241) (datanode2 executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ExecAbortError: Transaction discarded because of previous errors.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000016/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000016/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1910, in _execute_transaction\n",
      "    raise errors[0][1]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1900, in _execute_transaction\n",
      "    self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: Command # 2 (RPUSH spark_01_keys�\u0004�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000J�s{\u0000. \u0000\u0000\u0000\u0000\u0013��\u0000\u0000\u0000\u0000\u0001H\u001f\u0000\u0000\u0000\u0000)�u5\u0000\u0000\u0000\u0000\u000fW\u0006�\u0000\u0000\u0000\u0000\u0000��- \u0000\u0000\u0000\u0000#t\u0005�\u0000\u0000\u0000\u0000\n",
      "�B\\ \u0000\u0000\u0000\u0000\u0004�\u0000\u0000\u0000\u0000#\u0015��\u0000\u0000\u0000\u0000\u000e\u0015hF\u0000\u0000\u0000\u0000%��v\u0000\u0000\u0000\u00002�y#��\u0000\u0000\u0000\u0000���\t\u0000\u0000\u0000\u00003:��\u0000\u0000\u0000\u0000\u0012+\u0018� \u0000\u0000\u0000\u0000\u001d",
      "_o�\u0000\u0000\u0000\u0000\u0004���\u0000\u0000\u0000\u0000)��\u001f\u0000\u0000\u0000\u0000D!Ȃ\u0000\u0000\u0000\u0000OR/� \u0000\u0000\u0000\u0000V\u0005>`\u0000\u0000\u0000\u00004�\u0010\u0016\u0000\u0000\u0000\u0000\u001c",
      "��(\u0000\u0000\u0000\u0000+�q�\u0000\u0000\u0000\u0000&2EH \u0000\u0000\u0000\u0000tjS�\u0000\u0000\u0000\u0000\u00064�\u0019\u0000\u0000\u0000\u0000\u0014,7�\u0000\u0000\u0000\u0000�8e�\u0000\u0000\u0000\u0000\u0003\u0015oD \u0000\u0000\u0000\u0000\u0018�2\u000e\u0000\u0000\u0000\u0000�[��\u0000\u0000\u0000\u0000s\u0016��\u0000\u0000\u0000\u00007Y-K\u0000\u0000\u0000�ox \u0000\u0000\u0000t��\u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u0000z��}\u0000\u0000\u0000\u0000 \u0012��\u0000\u0000\u0000\u0000\n",
      "y�i\u0000\u0000\u0000\u0000\u001b��;\u0000\u0000\u0000\u0000 �\u0011$ \u0000\u0000\u0000\u0000&��P\u0000\u0000\u0000\u0000\u000b",
      "��b\u0000\u0000\u0000\u0000\u001f+��\u0000\u0000\u0000\u0000\u0005��\u001f\u0000\u0000\u0000\u00009�[� \u0000\u0000\u0000\u0000d��\u0000\u0000\u0000\u0000;\u0004\u0016|\u0000\u0000\u0000\u0000l�ї\u0000\u0000\u0000\u0000\u0013\t\"{\u0000\u0000\u0000\u0000*��7 \u0000\u0000\u0000\u0000$�{^\u0000\u0000\u0000\u0000\u0006\u001c",
      "�\u000f\u0000\u0000\u0000\u0000S�q�\u0000\u0000\u0000\u0000��'�\u0000\u0000\u0000\u0000���Z \u0000\u0000\u0000\u00008�iR\u0000\u0000\u0000\u0000\u0006̕�\u0000\u0000\u0000\u0000\u0001�o�\u0000\u0000\u0000\u0000'\u00058|\u0000\u0000\u0000\u0000<:�\u0018 \u0000\u0000\u0000\u0000kT�\u0016\u0000\u0000\u0000\u0000*�;\u0000\u0000\u0000\u00008�u�\u0000\u0000\u0000\u0000\u0003��,\u0000\u0000\u0000\u0000:�P� \u0000\u0000\u0000\u001e",
      "(\u001f\u0000\u0000\u0000\u0000\u0000��F\u0000\u0000\u0000\u0000i\\\u0017\t\u0000\u0000\u0000\u0000\u0018�E�\u0000\u0000\u0000\u0000�[4^ \u0000\u0000\u0000\u0000<\u001fݫ\u0000\u0000\u0000\u0000WJ>\u0000\u0000\u0000\u0000\u0000���+\u0000\u0000\u0000\u0000\t\u0004\u0003�\u0000\u0000\u0000\u0000�-:� \u0000\u0000\u0000\u0000!�*�\u0000\u0000\u0000\u0000k+�\\\u0000\u0000\u0000\u0000\u0001�\u001d",
      "\u001d",
      "\u0000\u0000\u0000\u0000~\f",
      "��\u0000\u0000\u0000\u0000\u0005Ч� \u0000\u0000\u0000\u0000I��\u0004\u0000\u0000\u0000\u0000\u0003��\u0005\u0000\u0000\u0000\u0000`�\u0014p\u0000\u0000\u0000\u0000\u0010��Y\u0000\u0000\u0000\u0000Pc� \u0000\u0000\u0000\u0000\u0000װH\u0000\u0000\u0000\u0000\u001e",
      "�y{\u0000\u0000\u0000\u0000\u001b\tu`\u0000\u0000\u0000\u0000\u0010s#�\u0000\u0000\u0000\u0000B\u000e�� \u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u00000�P_\u0000\u0000\u0000\u0000\"�l\\\u0000\u0000\u0000\u0000>[B�\u0000\u0000\u0000\u0000cO�6 \u0000\u0000\u0000\u0000M9��\u0000\u0000\u0000\u0000\u0004��`\u0000\u0000\u0000\u0000\u000e�w�\u0000\u0000\u0000\u00007�>�\u0000\u0000\u0000\u0000\u0012\u0001�{) of pipeline caused error: MOVED 4997 10.10.1.27:6379\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:06,340 WARN scheduler.TaskSetManager: Lost task 25.0 in stage 4.0 (TID 243) (datanode4 executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:06,371 WARN scheduler.TaskSetManager: Lost task 24.0 in stage 4.0 (TID 242) (datanode4 executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000009/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000009/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:06,417 WARN scheduler.TaskSetManager: Lost task 19.0 in stage 4.0 (TID 247) (datanode4 executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000013/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000013/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:06,432 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 4.0 (TID 248) (datanode4 executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000005/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000005/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:06,855 WARN scheduler.TaskSetManager: Lost task 27.0 in stage 4.0 (TID 256) (datanode2 executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n",
      "    response = self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ExecAbortError: Transaction discarded because of previous errors.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000008/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000008/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n",
      "    self.close()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n",
      "    self.lsh.keys.empty_buffer()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n",
      "    self._buffer.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n",
      "    return conn.retry.call_with_retry(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n",
      "    return do()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n",
      "    lambda: execute(conn, stack, raise_on_error),\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1910, in _execute_transaction\n",
      "    raise errors[0][1]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1900, in _execute_transaction\n",
      "    self.parse_response(connection, \"_\")\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n",
      "    result = Redis.parse_response(self, connection, command_name, **options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n",
      "    response = connection.read_response()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n",
      "    raise response\n",
      "redis.exceptions.ResponseError: Command # 2 (RPUSH spark_01_keys�\u0004�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000J�GU\u0002. \u0000\u0000\u0000\u0000\u0013��\u0000\u0000\u0000\u0000\u0001H\u001f\u0000\u0000\u0000\u0000)�u5\u0000\u0000\u0000\u0000\u000fW\u0006�\u0000\u0000\u0000\u0000\u0000��- \u0000\u0000\u0000\u0000#t\u0005�\u0000\u0000\u0000\u0000\n",
      "�B\\ \u0000\u0000\u0000\u0000\u0004�\u0000\u0000\u0000\u0000#\u0015��\u0000\u0000\u0000\u0000\u000e\u0015hF\u0000\u0000\u0000\u0000%��v\u0000\u0000\u0000\u00002�y#��\u0000\u0000\u0000\u0000���\t\u0000\u0000\u0000\u00003:��\u0000\u0000\u0000\u0000\u0012+\u0018� \u0000\u0000\u0000\u0000\u001d",
      "_o�\u0000\u0000\u0000\u0000\u0004���\u0000\u0000\u0000\u0000)��\u001f\u0000\u0000\u0000\u0000D!Ȃ\u0000\u0000\u0000\u0000OR/� \u0000\u0000\u0000\u0000V\u0005>`\u0000\u0000\u0000\u00004�\u0010\u0016\u0000\u0000\u0000\u0000\u001c",
      "��(\u0000\u0000\u0000\u0000+�q�\u0000\u0000\u0000\u0000&2EH \u0000\u0000\u0000\u0000tjS�\u0000\u0000\u0000\u0000\u00064�\u0019\u0000\u0000\u0000\u0000\u0014,7�\u0000\u0000\u0000\u0000�8e�\u0000\u0000\u0000\u0000\u0003\u0015oD \u0000\u0000\u0000\u0000\u0018�2\u000e\u0000\u0000\u0000\u0000�[��\u0000\u0000\u0000\u0000s\u0016��\u0000\u0000\u0000\u00007Y-K\u0000\u0000\u0000�ox \u0000\u0000\u0000t��\u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u0000z��}\u0000\u0000\u0000\u0000 \u0012��\u0000\u0000\u0000\u0000\n",
      "y�i\u0000\u0000\u0000\u0000\u001b��;\u0000\u0000\u0000\u0000 �\u0011$ \u0000\u0000\u0000\u0000&��P\u0000\u0000\u0000\u0000\u000b",
      "��b\u0000\u0000\u0000\u0000\u001f+��\u0000\u0000\u0000\u0000\u0005��\u001f\u0000\u0000\u0000\u00009�[� \u0000\u0000\u0000\u0000d��\u0000\u0000\u0000\u0000;\u0004\u0016|\u0000\u0000\u0000\u0000l�ї\u0000\u0000\u0000\u0000\u0013\t\"{\u0000\u0000\u0000\u0000*��7 \u0000\u0000\u0000\u0000$�{^\u0000\u0000\u0000\u0000\u0006\u001c",
      "�\u000f\u0000\u0000\u0000\u0000S�q�\u0000\u0000\u0000\u0000��'�\u0000\u0000\u0000\u0000���Z \u0000\u0000\u0000\u00008�iR\u0000\u0000\u0000\u0000\u0006̕�\u0000\u0000\u0000\u0000\u0001�o�\u0000\u0000\u0000\u0000'\u00058|\u0000\u0000\u0000\u0000<:�\u0018 \u0000\u0000\u0000\u0000kT�\u0016\u0000\u0000\u0000\u0000*�;\u0000\u0000\u0000\u00008�u�\u0000\u0000\u0000\u0000\u0003��,\u0000\u0000\u0000\u0000:�P� \u0000\u0000\u0000\u001e",
      "(\u001f\u0000\u0000\u0000\u0000\u0000��F\u0000\u0000\u0000\u0000i\\\u0017\t\u0000\u0000\u0000\u0000\u0018�E�\u0000\u0000\u0000\u0000�[4^ \u0000\u0000\u0000\u0000<\u001fݫ\u0000\u0000\u0000\u0000WJ>\u0000\u0000\u0000\u0000\u0000���+\u0000\u0000\u0000\u0000\t\u0004\u0003�\u0000\u0000\u0000\u0000�-:� \u0000\u0000\u0000\u0000!�*�\u0000\u0000\u0000\u0000k+�\\\u0000\u0000\u0000\u0000\u0001�\u001d",
      "\u001d",
      "\u0000\u0000\u0000\u0000~\f",
      "��\u0000\u0000\u0000\u0000\u0005Ч� \u0000\u0000\u0000\u0000I��\u0004\u0000\u0000\u0000\u0000\u0003��\u0005\u0000\u0000\u0000\u0000`�\u0014p\u0000\u0000\u0000\u0000\u0010��Y\u0000\u0000\u0000\u0000Pc� \u0000\u0000\u0000\u0000\u0000װH\u0000\u0000\u0000\u0000\u001e",
      "�y{\u0000\u0000\u0000\u0000\u001b\tu`\u0000\u0000\u0000\u0000\u0010s#�\u0000\u0000\u0000\u0000B\u000e�� \u0000\u0000\u0000\u0000!���\u0000\u0000\u0000\u00000�P_\u0000\u0000\u0000\u0000\"�l\\\u0000\u0000\u0000\u0000>[B�\u0000\u0000\u0000\u0000cO�6 \u0000\u0000\u0000\u0000M9��\u0000\u0000\u0000\u0000\u0004��`\u0000\u0000\u0000\u0000\u000e�w�\u0000\u0000\u0000\u00007�>�\u0000\u0000\u0000\u0000\u0012\u0001�{) of pipeline caused error: MOVED 4344 10.10.1.27:6379\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 13:24:06,931 ERROR scheduler.TaskSetManager: Task 3 in stage 4.0 failed 4 times; aborting job\n",
      "2022-03-31 13:24:06,974 WARN scheduler.TaskSetManager: Lost task 17.0 in stage 4.0 (TID 269) (datanode3 executor 9): TaskKilled (Stage cancelled)\n",
      "2022-03-31 13:24:06,994 WARN scheduler.TaskSetManager: Lost task 22.0 in stage 4.0 (TID 252) (datanode1 executor 2): TaskKilled (Stage cancelled)\n",
      "2022-03-31 13:24:07,016 WARN scheduler.TaskSetManager: Lost task 9.2 in stage 4.0 (TID 260) (datanode4 executor 8): TaskKilled (Stage cancelled)\n",
      "2022-03-31 13:24:07,035 WARN scheduler.TaskSetManager: Lost task 6.2 in stage 4.0 (TID 261) (datanode4 executor 12): TaskKilled (Stage cancelled)\n",
      "2022-03-31 13:24:07,035 WARN scheduler.TaskSetManager: Lost task 7.3 in stage 4.0 (TID 262) (datanode3 executor 1): TaskKilled (Stage cancelled)\n",
      "2022-03-31 13:24:07,124 WARN scheduler.TaskSetManager: Lost task 23.1 in stage 4.0 (TID 263) (datanode4 executor 4): TaskKilled (Stage cancelled)\n",
      "2022-03-31 13:24:07,226 WARN scheduler.TaskSetManager: Lost task 2.2 in stage 4.0 (TID 265) (datanode3 executor 5): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 4.0 failed 4 times, most recent failure: Lost task 3.3 in stage 4.0 (TID 250) (datanode3 executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000010/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000010/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  [Previous line repeated 1 more time]\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n    self.close()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n    self.lsh.keys.empty_buffer()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n    self._buffer.execute()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n    return conn.retry.call_with_retry(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n    return do()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n    lambda: execute(conn, stack, raise_on_error),\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n    response = self.parse_response(connection, \"_\")\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n    result = Redis.parse_response(self, connection, command_name, **options)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n    response = connection.read_response()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n    raise response\nredis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000010/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000010/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  [Previous line repeated 1 more time]\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n    self.close()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n    self.lsh.keys.empty_buffer()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n    self._buffer.execute()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n    return conn.retry.call_with_retry(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n    return do()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n    lambda: execute(conn, stack, raise_on_error),\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n    response = self.parse_response(connection, \"_\")\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n    result = Redis.parse_response(self, connection, command_name, **options)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n    response = connection.read_response()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n    raise response\nredis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m lsh\u001b[38;5;241m.\u001b[39minsertion_session() \u001b[38;5;28;01mas\u001b[39;00m sess:\n\u001b[1;32m     12\u001b[0m             sess\u001b[38;5;241m.\u001b[39minsert(i, v)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mhashes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachPartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43muploadFunction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:938\u001b[0m, in \u001b[0;36mRDD.foreachPartition\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([])\n\u001b[0;32m--> 938\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1237\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1226\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1080\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 1080\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:950\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 950\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 4.0 failed 4 times, most recent failure: Lost task 3.3 in stage 4.0 (TID 250) (datanode3 executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000010/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000010/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  [Previous line repeated 1 more time]\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n    self.close()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n    self.lsh.keys.empty_buffer()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n    self._buffer.execute()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n    return conn.retry.call_with_retry(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n    return do()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n    lambda: execute(conn, stack, raise_on_error),\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n    response = self.parse_response(connection, \"_\")\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n    result = Redis.parse_response(self, connection, command_name, **options)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n    response = connection.read_response()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n    raise response\nredis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000010/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0015/container_1648460271792_0015_01_000010/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  [Previous line repeated 1 more time]\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n  File \"/tmp/ipykernel_311748/1692678216.py\", line 12, in uploadFunction\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 347, in __exit__\n    self.close()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 350, in close\n    self.lsh.keys.empty_buffer()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 1064, in empty_buffer\n    self._buffer.execute()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2035, in execute\n    return conn.retry.call_with_retry(\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/retry.py\", line 45, in call_with_retry\n    return do()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 2036, in <lambda>\n    lambda: execute(conn, stack, raise_on_error),\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1907, in _execute_transaction\n    response = self.parse_response(connection, \"_\")\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1975, in parse_response\n    result = Redis.parse_response(self, connection, command_name, **options)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/client.py\", line 1211, in parse_response\n    response = connection.read_response()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/redis/connection.py\", line 836, in read_response\n    raise response\nredis.exceptions.ResponseError: CROSSSLOT Keys in request don't hash to the same slot\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 13:24:07,270 WARN scheduler.TaskSetManager: Lost task 28.2 in stage 4.0 (TID 264) (datanode2 executor 3): TaskKilled (Stage cancelled)\n",
      "2022-03-31 13:24:07,302 WARN scheduler.TaskSetManager: Lost task 27.1 in stage 4.0 (TID 267) (datanode2 executor 7): TaskKilled (Stage cancelled)\n",
      "2022-03-31 13:24:07,312 WARN scheduler.TaskSetManager: Lost task 10.3 in stage 4.0 (TID 266) (datanode2 executor 11): TaskKilled (Stage cancelled)\n",
      "2022-03-31 13:24:07,348 WARN scheduler.TaskSetManager: Lost task 0.2 in stage 4.0 (TID 268) (datanode1 executor 13): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "def uploadFunction(variables):\n",
    "    lsh = ds.MinHashLSH(threshold=0.5, num_perm=128, storage_config={\n",
    "            'type': 'redis',\n",
    "            'redis': {'host': '10.10.1.220', 'port': 6379},\n",
    "            'basename': b'spark_01'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    h = map(lambda x: (x[0], ds.LeanMinHash(seed=x[1][0], hashvalues=x[1][1])), variables)\n",
    "    for i, v in h:\n",
    "        with lsh.insertion_session() as sess:\n",
    "            sess.insert(i, v)\n",
    "\n",
    "hashes.foreachPartition(uploadFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df6945-4b10-4854-ae89-d36667bb026d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
