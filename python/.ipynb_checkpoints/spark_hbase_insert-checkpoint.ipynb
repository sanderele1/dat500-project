{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbdef26d-8a7b-49fa-a711-e1bde7d7ff37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark/python/pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41efd6b-cc78-4f3a-b2ed-d222a717f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-03-31 11:43:01,977 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-03-31 11:43:09,017 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"yarn\")\n",
    "         .appName(\"python-testing\")\n",
    "         .config(\"spark.executor.instances\", 16)\n",
    "         .config(\"spark.executor.memory\", \"1536m\")\n",
    "         #.config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "         #.config(\"spark.executor.cores\", 1)\n",
    "         #.config(\"spark.dynamicAllocation.minExecutors\", 4)\n",
    "         #.config(\"spark.dynamicAllocation.maxExecutors\", 32)\n",
    "         #.config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "         #.config(\"spark.shuffle.service.port\", 7337)\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa94b247-89c2-439f-8f81-80bb7fe95edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.environment['PYTHONPATH'] = '/home/ubuntu/.local/lib/python3.8/site-packages'\n",
    "sc.addPyFile('hbase_datasketch.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b7ec69-940a-4d9d-9d50-3ab593703474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/spark/python',\n",
       " '/tmp/spark-697b8779-b74c-4a77-a31b-a24e83e6ba69/userFiles-67bef132-da5c-4541-8b0f-b2d51db0b30d',\n",
       " '/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip',\n",
       " '/home/ubuntu/project/python',\n",
       " '/usr/lib/python38.zip',\n",
       " '/usr/lib/python3.8',\n",
       " '/usr/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/ubuntu/.local/lib/python3.8/site-packages',\n",
       " '/usr/local/lib/python3.8/dist-packages',\n",
       " '/usr/lib/python3/dist-packages']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876c6fc4-ad86-46af-8634-f9b32fbbeabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(30126923,\n",
       "  'CGCATCGTGGCTGGACCTGAGTCCATCTGCCCTGGTGCCTGCATGACTGGCCCTTCTCCTTCACAGACCATGGCCCCAGGCTCCCTTGCTTTCATTTCCCAGCCCGTTATTGGGGCAGGAGAGTAGCAAGCGGGGGAGTTTTGATGAGGCGAGGA')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = sc.sequenceFile(\"hdfs:///files/windowed\")\n",
    "windows.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1d85be7-f212-4af5-a8a2-ac94b1bd5737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29288"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = windows.sample(fraction=0.0005, withReplacement=False, seed=1).cache()\n",
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245782b1-76c1-437e-8d59-e90d460b0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hbase_datasketch\n",
    "import happybase as hb\n",
    "import datasketch as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "404626c3-702b-4ed9-ac8b-6a0412ba5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashValue(value):\n",
    "    import datasketch as ds\n",
    "    h = ds.MinHash()\n",
    "    for v in value:\n",
    "        h.update(v.encode('utf8'))\n",
    "    return (h.seed, h.hashvalues)\n",
    "\n",
    "hashes = sample.mapValues(hashValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e898daa-dec5-4f8f-b245-87459ed496c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 11:45:01,715 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 3.0 (TID 143) (datanode4 executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAM: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:06,961 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 3.0 (TID 135) (datanode4 executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:09,186 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 3.0 (TID 140) (datanode2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:11,325 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 3.0 (TID 130) (datanode2 executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:14,547 WARN scheduler.TaskSetManager: Lost task 13.0 in stage 3.0 (TID 144) (datanode4 executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:18,816 WARN scheduler.TaskSetManager: Lost task 12.0 in stage 3.0 (TID 132) (datanode4 executor 11): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAD: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:19,945 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 3.0 (TID 138) (datanode1 executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:954)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:832)\\n\\tat org.apache.hadoop.hbase.client.HRegionLocator.getRegionLocation(HRegionLocator.java:64)\\n\\tat org.apache.hadoop.hbase.client.RegionLocator.getRegionLocation(RegionLocator.java:70)\\n\\tat org.apache.hadoop.hbase.client.RegionLocator.getRegionLocation(RegionLocator.java:59)\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:223)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:21,083 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 3.0 (TID 142) (datanode1 executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:21,095 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 3.0 (TID 139) (datanode2 executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:21,117 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 3.0 (TID 137) (datanode1 executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:21,129 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 3.0 (TID 136) (datanode1 executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:21,147 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 3.0 (TID 141) (datanode3 executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:21,152 WARN scheduler.TaskSetManager: Lost task 15.0 in stage 3.0 (TID 131) (datanode3 executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:21,160 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 3.0 (TID 133) (datanode3 executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:23,280 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 3.0 (TID 134) (datanode3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAC: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:49,188 WARN scheduler.TaskSetManager: Lost task 19.0 in stage 3.0 (TID 145) (datanode4 executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:52,989 WARN scheduler.TaskSetManager: Lost task 32.0 in stage 3.0 (TID 146) (datanode4 executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAJ: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:55,446 WARN scheduler.TaskSetManager: Lost task 22.0 in stage 3.0 (TID 147) (datanode2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:45:58,621 WARN scheduler.TaskSetManager: Lost task 20.0 in stage 3.0 (TID 148) (datanode2 executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:00,498 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 3.0 (TID 149) (datanode4 executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:05,858 WARN scheduler.TaskSetManager: Lost task 25.0 in stage 3.0 (TID 150) (datanode4 executor 11): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:07,594 WARN scheduler.TaskSetManager: Lost task 15.1 in stage 3.0 (TID 157) (datanode3 executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:10,160 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 3.0 (TID 154) (datanode1 executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:954)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:832)\\n\\tat org.apache.hadoop.hbase.client.HRegionLocator.getRegionLocation(HRegionLocator.java:64)\\n\\tat org.apache.hadoop.hbase.client.RegionLocator.getRegionLocation(RegionLocator.java:70)\\n\\tat org.apache.hadoop.hbase.client.RegionLocator.getRegionLocation(RegionLocator.java:59)\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:223)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:12,067 WARN scheduler.TaskSetManager: Lost task 26.0 in stage 3.0 (TID 151) (datanode1 executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:14,541 WARN scheduler.TaskSetManager: Lost task 5.1 in stage 3.0 (TID 153) (datanode2 executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAE: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:14,562 WARN scheduler.TaskSetManager: Lost task 11.1 in stage 3.0 (TID 155) (datanode1 executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAE: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:14,572 WARN scheduler.TaskSetManager: Lost task 29.0 in stage 3.0 (TID 156) (datanode3 executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAE: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:16,377 WARN scheduler.TaskSetManager: Lost task 7.1 in stage 3.0 (TID 158) (datanode3 executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:16,861 WARN scheduler.TaskSetManager: Lost task 14.0 in stage 3.0 (TID 152) (datanode1 executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:16,876 WARN scheduler.TaskSetManager: Lost task 31.0 in stage 3.0 (TID 159) (datanode3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:34,204 WARN scheduler.TaskSetManager: Lost task 9.1 in stage 3.0 (TID 160) (datanode4 executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAF: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:39,588 WARN scheduler.TaskSetManager: Lost task 6.1 in stage 3.0 (TID 161) (datanode4 executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:42,544 WARN scheduler.TaskSetManager: Lost task 1.1 in stage 3.0 (TID 162) (datanode2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.getTableState(ConnectionImplementation.java:2158)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.isTableDisabled(ConnectionImplementation.java:678)\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:219)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:44,125 WARN scheduler.TaskSetManager: Lost task 8.1 in stage 3.0 (TID 163) (datanode2 executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:49,354 WARN scheduler.TaskSetManager: Lost task 13.1 in stage 3.0 (TID 164) (datanode4 executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:51,072 WARN scheduler.TaskSetManager: Lost task 18.0 in stage 3.0 (TID 166) (datanode3 executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:54,313 WARN scheduler.TaskSetManager: Lost task 12.1 in stage 3.0 (TID 165) (datanode4 executor 11): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:46:58,784 WARN scheduler.TaskSetManager: Lost task 10.1 in stage 3.0 (TID 168) (datanode1 executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:00,761 WARN scheduler.TaskSetManager: Lost task 17.0 in stage 3.0 (TID 172) (datanode3 executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:02,171 WARN scheduler.TaskSetManager: Lost task 24.0 in stage 3.0 (TID 167) (datanode1 executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:04,367 WARN scheduler.TaskSetManager: Lost task 31.1 in stage 3.0 (TID 174) (datanode3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:07,282 WARN scheduler.TaskSetManager: Lost task 2.1 in stage 3.0 (TID 171) (datanode3 executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:954)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:832)\\n\\tat org.apache.hadoop.hbase.client.HRegionLocator.getRegionLocation(HRegionLocator.java:64)\\n\\tat org.apache.hadoop.hbase.client.RegionLocator.getRegionLocation(RegionLocator.java:70)\\n\\tat org.apache.hadoop.hbase.client.RegionLocator.getRegionLocation(RegionLocator.java:59)\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:223)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:07,334 WARN scheduler.TaskSetManager: Lost task 21.0 in stage 3.0 (TID 169) (datanode2 executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.getTableState(ConnectionImplementation.java:2158)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.isTableDisabled(ConnectionImplementation.java:678)\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:219)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:10,334 WARN scheduler.TaskSetManager: Lost task 30.0 in stage 3.0 (TID 170) (datanode1 executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:10,348 WARN scheduler.TaskSetManager: Lost task 4.1 in stage 3.0 (TID 173) (datanode1 executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:30,787 WARN scheduler.TaskSetManager: Lost task 19.1 in stage 3.0 (TID 175) (datanode4 executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAF: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:33,061 WARN scheduler.TaskSetManager: Lost task 32.1 in stage 3.0 (TID 176) (datanode4 executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:36,401 WARN scheduler.TaskSetManager: Lost task 22.1 in stage 3.0 (TID 177) (datanode2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:38,927 WARN scheduler.TaskSetManager: Lost task 20.1 in stage 3.0 (TID 178) (datanode2 executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.getTableState(ConnectionImplementation.java:2158)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.isTableDisabled(ConnectionImplementation.java:678)\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:219)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:40,742 WARN scheduler.TaskSetManager: Lost task 15.2 in stage 3.0 (TID 180) (datanode3 executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAK: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:44,961 WARN scheduler.TaskSetManager: Lost task 23.1 in stage 3.0 (TID 179) (datanode4 executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAC: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:48,002 WARN scheduler.TaskSetManager: Lost task 25.1 in stage 3.0 (TID 181) (datanode4 executor 11): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAC: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:50,949 WARN scheduler.TaskSetManager: Lost task 7.2 in stage 3.0 (TID 183) (datanode3 executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotFoundException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.getTableState(ConnectionImplementation.java:2158)\\n\\tat org.apache.hadoop.hbase.client.ConnectionImplementation.isTableDisabled(ConnectionImplementation.java:678)\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:219)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:53,281 WARN scheduler.TaskSetManager: Lost task 26.1 in stage 3.0 (TID 182) (datanode1 executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:56,512 WARN scheduler.TaskSetManager: Lost task 3.1 in stage 3.0 (TID 185) (datanode3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:47:56,614 WARN scheduler.TaskSetManager: Lost task 0.2 in stage 3.0 (TID 184) (datanode1 executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:00,767 WARN scheduler.TaskSetManager: Lost task 29.1 in stage 3.0 (TID 186) (datanode3 executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAN: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:02,978 WARN scheduler.TaskSetManager: Lost task 5.2 in stage 3.0 (TID 187) (datanode2 executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:05,585 WARN scheduler.TaskSetManager: Lost task 4.2 in stage 3.0 (TID 189) (datanode1 executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:05,596 WARN scheduler.TaskSetManager: Lost task 11.2 in stage 3.0 (TID 188) (datanode1 executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:22,876 WARN scheduler.TaskSetManager: Lost task 8.2 in stage 3.0 (TID 193) (datanode2 executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000010/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAF: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:24,645 WARN scheduler.TaskSetManager: Lost task 6.2 in stage 3.0 (TID 191) (datanode4 executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000008/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:24,668 WARN scheduler.TaskSetManager: Lost task 9.2 in stage 3.0 (TID 190) (datanode4 executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000016/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:29,065 WARN scheduler.TaskSetManager: Lost task 18.1 in stage 3.0 (TID 194) (datanode3 executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000007/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:30,050 WARN scheduler.TaskSetManager: Lost task 1.2 in stage 3.0 (TID 192) (datanode2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000002/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:31,259 WARN scheduler.TaskSetManager: Lost task 13.2 in stage 3.0 (TID 195) (datanode4 executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000004/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAA: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:34,566 WARN scheduler.TaskSetManager: Lost task 12.2 in stage 3.0 (TID 196) (datanode4 executor 11): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000012/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:39,699 WARN scheduler.TaskSetManager: Lost task 10.2 in stage 3.0 (TID 198) (datanode1 executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000005/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAM: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:40,684 WARN scheduler.TaskSetManager: Lost task 17.1 in stage 3.0 (TID 197) (datanode3 executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000011/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:45,349 WARN scheduler.TaskSetManager: Lost task 24.1 in stage 3.0 (TID 200) (datanode1 executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000009/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAC: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:45,993 WARN scheduler.TaskSetManager: Lost task 31.2 in stage 3.0 (TID 199) (datanode3 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000003/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAB: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:48,307 WARN scheduler.TaskSetManager: Lost task 2.2 in stage 3.0 (TID 201) (datanode3 executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000015/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:51,525 WARN scheduler.TaskSetManager: Lost task 14.1 in stage 3.0 (TID 203) (datanode1 executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 167, in _insert\n",
      "    if check_duplication and key in self.keys:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/storage.py\", line 118, in __contains__\n",
      "    return self.has_key(item)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000013/hbase_datasketch.py\", line 85, in has_key\n",
      "    row = table.row(key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/table.py\", line 121, in row\n",
      "    rows = self.connection.client.getRowWithColumns(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.TableNotEnabledException: aW5qZWN0aW9uX3Rlc3Rfa2V5cw is disabled.\\n\\tat org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:220)\\n\\tat org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:106)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:382)\\n\\tat org.apache.hadoop.hbase.client.HTable.get(HTable.java:356)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumnsTs(ThriftHBaseServiceHandler.java:477)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.getRowWithColumns(ThriftHBaseServiceHandler.java:453)\\n\\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.getRowWithColumns(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4290)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getRowWithColumns.getResult(Hbase.java:4269)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:51,583 WARN scheduler.TaskSetManager: Lost task 21.1 in stage 3.0 (TID 202) (datanode2 executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000006/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAD: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:55,772 WARN scheduler.TaskSetManager: Lost task 11.3 in stage 3.0 (TID 204) (datanode1 executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n",
      "  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n",
      "    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n",
      "    hashtable.insert(H, key, buffer=buffer)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 104, in insert\n",
      "    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n",
      "  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 72, in insert\n",
      "    batch.put(binary_key + value, {b'fvalue:value': value})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n",
      "    self.send()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n",
      "    self._table.connection.client.mutateRows(self._table.name, bms, {})\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n",
      "    return self._recv(_api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n",
      "    raise v\n",
      "Hbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAD: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-31 11:48:55,783 ERROR scheduler.TaskSetManager: Task 11 in stage 3.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 3.0 failed 4 times, most recent failure: Lost task 11.3 in stage 3.0 (TID 204) (datanode1 executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  [Previous line repeated 1 more time]\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n    hashtable.insert(H, key, buffer=buffer)\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 104, in insert\n    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 72, in insert\n    batch.put(binary_key + value, {b'fvalue:value': value})\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n    self.send()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n    self._table.connection.client.mutateRows(self._table.name, bms, {})\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n    return self._recv(_api)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n    raise v\nHbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAD: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  [Previous line repeated 1 more time]\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n    hashtable.insert(H, key, buffer=buffer)\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 104, in insert\n    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 72, in insert\n    batch.put(binary_key + value, {b'fvalue:value': value})\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n    self.send()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n    self._table.connection.client.mutateRows(self._table.name, bms, {})\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n    return self._recv(_api)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n    raise v\nHbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAD: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m h:\n\u001b[1;32m     19\u001b[0m             lsh\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, value)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mhashes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachPartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43muploadPartition\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:938\u001b[0m, in \u001b[0;36mRDD.foreachPartition\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([])\n\u001b[0;32m--> 938\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1237\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1226\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1080\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 1080\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:950\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 950\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 3.0 failed 4 times, most recent failure: Lost task 11.3 in stage 3.0 (TID 204) (datanode1 executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  [Previous line repeated 1 more time]\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n    hashtable.insert(H, key, buffer=buffer)\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 104, in insert\n    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 72, in insert\n    batch.put(binary_key + value, {b'fvalue:value': value})\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n    self.send()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n    self._table.connection.client.mutateRows(self._table.name, bms, {})\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n    return self._recv(_api)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n    raise v\nHbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAD: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2918, in pipeline_func\n  [Previous line repeated 1 more time]\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 417, in func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 933, in func\n  File \"/tmp/ipykernel_310479/3940535762.py\", line 19, in uploadPartition\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 148, in insert\n    self._insert(key, minhash, check_duplication=check_duplication, buffer=False)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/datasketch/lsh.py\", line 173, in _insert\n    hashtable.insert(H, key, buffer=buffer)\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 104, in insert\n    HBaseDictListStorage.insert(self, key, *vals, **kwargs)\n  File \"/tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1648460271792_0013/container_1648460271792_0013_01_000017/hbase_datasketch.py\", line 72, in insert\n    batch.put(binary_key + value, {b'fvalue:value': value})\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 137, in __exit__\n    self.send()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/happybase/batch.py\", line 60, in send\n    self._table.connection.client.mutateRows(self._table.name, bms, {})\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 219, in _req\n    return self._recv(_api)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/thriftpy2/thrift.py\", line 251, in _recv\n    raise v\nHbase_thrift.IOError: IOError(message=b'org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: aW5qZWN0aW9uX3Rlc3RfYnVja2V0XwAD: 1 time, servers with issues: null\\n\\tat org.apache.hadoop.hbase.client.BatchErrors.makeException(BatchErrors.java:54)\\n\\tat org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.getErrors(AsyncRequestFutureImpl.java:1196)\\n\\tat org.apache.hadoop.hbase.client.HTable.batch(HTable.java:451)\\n\\tat org.apache.hadoop.hbase.client.HTable.put(HTable.java:549)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRowsTs(ThriftHBaseServiceHandler.java:814)\\n\\tat org.apache.hadoop.hbase.thrift.ThriftHBaseServiceHandler.mutateRows(ThriftHBaseServiceHandler.java:748)\\n\\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.invoke(HbaseHandlerMetricsProxy.java:72)\\n\\tat com.sun.proxy.$Proxy10.mutateRows(Unknown Source)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4555)\\n\\tat org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRows.getResult(Hbase.java:4534)\\n\\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\\n\\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)\\n\\tat org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer$ClientConnnection.run(TBoundedThreadPoolServer.java:297)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n')\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "def uploadPartition(hashes):\n",
    "    import datasketch as ds\n",
    "    import hbase_datasketch\n",
    "    import happybase as hb\n",
    "    h = map(lambda x: (x[0], ds.LeanMinHash(seed=x[1][0], hashvalues=x[1][1])), hashes)\n",
    "    \n",
    "    pool = hb.ConnectionPool(10, host='datanode2')\n",
    "    config = {'type': 'hbase',\n",
    "              'hbase_table': 'injection_test',\n",
    "              'hbase_pool': pool,\n",
    "              'hbase_keysize': 40,\n",
    "              'basename': b'',\n",
    "              'hbase_recreate_table': True}\n",
    "    \n",
    "    lsh = ds.MinHashLSH(threshold=0.7, storage_config=config)\n",
    "    \n",
    "    with lsh.insertion_session() as sess:\n",
    "        for i, value in h:\n",
    "            lsh.insert(f\"{i}\", value)\n",
    "\n",
    "hashes.foreachPartition(uploadPartition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0f2bc-b514-422d-8153-9723c7497889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 11:48:58,125 WARN scheduler.TaskSetManager: Lost task 3.2 in stage 3.0 (TID 215) (datanode3 executor 2): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:58,146 WARN scheduler.TaskSetManager: Lost task 22.2 in stage 3.0 (TID 209) (datanode2 executor 1): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:58,366 WARN scheduler.TaskSetManager: Lost task 29.2 in stage 3.0 (TID 216) (datanode3 executor 13): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:58,646 WARN scheduler.TaskSetManager: Lost task 25.2 in stage 3.0 (TID 211) (datanode4 executor 11): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:58,710 WARN scheduler.TaskSetManager: Lost task 32.2 in stage 3.0 (TID 206) (datanode4 executor 7): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:58,727 WARN scheduler.TaskSetManager: Lost task 19.2 in stage 3.0 (TID 207) (datanode4 executor 14): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:58,735 WARN scheduler.TaskSetManager: Lost task 7.3 in stage 3.0 (TID 213) (datanode3 executor 10): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:58,941 WARN scheduler.TaskSetManager: Lost task 20.2 in stage 3.0 (TID 205) (datanode2 executor 9): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:59,115 WARN scheduler.TaskSetManager: Lost task 15.3 in stage 3.0 (TID 208) (datanode3 executor 6): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:59,325 WARN scheduler.TaskSetManager: Lost task 23.2 in stage 3.0 (TID 210) (datanode4 executor 3): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:59,403 WARN scheduler.TaskSetManager: Lost task 0.3 in stage 3.0 (TID 214) (datanode1 executor 8): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:59,572 WARN scheduler.TaskSetManager: Lost task 4.3 in stage 3.0 (TID 217) (datanode1 executor 12): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:59,642 WARN scheduler.TaskSetManager: Lost task 5.3 in stage 3.0 (TID 218) (datanode2 executor 5): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:59,761 WARN scheduler.TaskSetManager: Lost task 26.2 in stage 3.0 (TID 212) (datanode1 executor 4): TaskKilled (Stage cancelled)\n",
      "2022-03-31 11:48:59,825 WARN scheduler.TaskSetManager: Lost task 30.1 in stage 3.0 (TID 219) (datanode1 executor 15): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "pool = hb.ConnectionPool(10, host='datanode2')\n",
    "config = {'type': 'hbase',\n",
    "          'hbase_table': 'injection_test',\n",
    "          'hbase_pool': pool,\n",
    "          'hbase_keysize': 40,\n",
    "          'basename': b'',\n",
    "          'hbase_recreate_table': False}\n",
    "\n",
    "lsh = ds.MinHashLSH(threshold=0.7, storage_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57383031-212f-43b5-8d2b-ff4453d7def9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
