{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08dda8dc-64f3-48c9-855c-cda2d6d6d160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark/python/pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f82b2fa-1bc6-46c9-b86d-0e99176f197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-03-31 13:17:10,867 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-03-31 13:17:17,276 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"yarn\")\n",
    "         .appName(\"python-testing\")\n",
    "         .config(\"spark.executor.instances\", 16)\n",
    "         .config(\"spark.executor.memory\", \"1536m\")\n",
    "         #.config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "         #.config(\"spark.executor.cores\", 1)\n",
    "         #.config(\"spark.dynamicAllocation.minExecutors\", 4)\n",
    "         #.config(\"spark.dynamicAllocation.maxExecutors\", 32)\n",
    "         #.config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "         #.config(\"spark.shuffle.service.port\", 7337)\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afe445b-5204-41d9-8ebd-2d6a5899194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.environment['PYTHONPATH'] = '/home/ubuntu/.local/lib/python3.8/site-packages'\n",
    "sc.addPyFile('hbase_datasketch.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6ddb11-7b16-4de7-8934-bac7d5a7eaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(30126923,\n",
       "  'CGCATCGTGGCTGGACCTGAGTCCATCTGCCCTGGTGCCTGCATGACTGGCCCTTCTCCTTCACAGACCATGGCCCCAGGCTCCCTTGCTTTCATTTCCCAGCCCGTTATTGGGGCAGGAGAGTAGCAAGCGGGGGAGTTTTGATGAGGCGAGGA')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = sc.sequenceFile(\"hdfs:///files/windowed\")\n",
    "windows.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288d82d3-c2ba-45dc-a6d1-48b4ba60dce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = windows.sample(fraction=0.0005, withReplacement=False, seed=1).cache()\n",
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afd8465-e068-439c-b3c4-8224ba15f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasketch as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a45f531-0648-4e32-a53c-7fd6dda28513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashValue(value):\n",
    "    import datasketch as ds\n",
    "    h = ds.MinHash()\n",
    "    for v in value:\n",
    "        h.update(v.encode('utf8'))\n",
    "    return (h.seed, h.hashvalues)\n",
    "\n",
    "hashes = sample.mapValues(hashValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c50d9036-f56f-4e15-9139-8d53c3b55c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh = ds.MinHashLSH(threshold=0.5, num_perm=128, storage_config={\n",
    "        'type': 'redis',\n",
    "        'redis': {'host': 'datanode1', 'port': 6379},\n",
    "        'basename': b'spark_01'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da44af2-b15b-43d4-b921-91c226c38bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploadFunction(variables)\n",
    "    lsh = ds.MinHashLSH(threshold=0.5, num_perm=128, storage_config={\n",
    "            'type': 'redis',\n",
    "            'redis': {'host': 'datanode1', 'port': 6379},\n",
    "            'basename': b'spark_01'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    h = map(lambda x: (x[0], ds.LeanMinHash(seed=x[1][0], hashvalues=x[1][1])), variables)\n",
    "    for i, v in h:\n",
    "        with lsh.insertion_session() as sess:\n",
    "            sess.insert(i, v)\n",
    "\n",
    "hashes.foreachPartition(uploadFunction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
