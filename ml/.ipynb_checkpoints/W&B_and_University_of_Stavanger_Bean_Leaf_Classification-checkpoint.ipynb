{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn2AsN_8OF9C"
   },
   "source": [
    "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "<!--- @wandbcode{beans-University-of-Stavanger} -->\n",
    "\n",
    "Use Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\n",
    "\n",
    "\n",
    "<img src=\"https://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />\n",
    "\n",
    "\n",
    "## What this notebook covers with Weights and Biases:\n",
    "* Metrics logging \n",
    "* Exploratory Data Analysis (EDA)\n",
    "* W&B plots such as Confusion Matrices, ROC curves & PR curves\n",
    "* HyperParameter search with W&B Sweeps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNu4MLLEWoGm"
   },
   "source": [
    "# ‚úÖ Sign Up\n",
    "\n",
    "Sign up to a free [Weights & Biases account here](https://wandb.ai/signup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_vAHpRh9hi2"
   },
   "source": [
    "# Kaggle Competition Page\n",
    "\n",
    "[Submit to the Competition here](http://www.kaggle.com/c/university-of-stavanger-beans-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFmx5yiSHodB"
   },
   "source": [
    "# üöÄ Installing and importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "32x5NdqZE2e1"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade wandb\n",
    "!pip install -q scikit-learn==1.0.1\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PQV_RY7nauWI"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19z3oXf7t6Ng"
   },
   "source": [
    "A useful logging function to log multiple metrics to W&B at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwReo5oqkqbh"
   },
   "outputs": [],
   "source": [
    "def log_metrics(labels, preds, is_val=True):\n",
    "  if is_val: pref = 'validation'\n",
    "  else: pref = 'train'\n",
    "  \n",
    "  metrics = {}\n",
    "  metrics[f\"{pref}/accuracy_score\"] = accuracy_score(y_val, y_pred)\n",
    "  metrics[f\"{pref}/precision\"] = precision_score(y_val, y_pred, average=\"weighted\")\n",
    "  metrics[f\"{pref}/recall\"] = recall_score(y_val, y_pred, average=\"weighted\")\n",
    "  metrics[f\"{pref}/f1_score\"] = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "\n",
    "  for k in metrics.keys():\n",
    "    print(f'{k} : {metrics[k]}')\n",
    "    wandb.summary[f\"{k}\"] = metrics[k]\n",
    "\n",
    "  #wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK-8urUht2VY"
   },
   "source": [
    "Set some constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECCOZJy_Lx5O"
   },
   "outputs": [],
   "source": [
    "PROJECT = 'beans-tabular-University-of-Stavanger'\n",
    "DATA_DIR = 'data'\n",
    "ARTIFACT_PATH = 'wandb_fc/beans-tabular-pydata-tunisia/beans_competition_dataset:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHlJ_rz_Izr7"
   },
   "source": [
    "# üíæ Data\n",
    "#### Download and Load the Data\n",
    "`train.csv` and `val.csv` data will be downloaded to `DATA_DIR`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxu2Zl3wHpcU"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=PROJECT, job_type='download_dataset')\n",
    "artifact = wandb.use_artifact(ARTIFACT_PATH, type='dataset')\n",
    "artifact_dir = artifact.download(DATA_DIR)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baxhbeeeJOM1"
   },
   "outputs": [],
   "source": [
    "# Read csvs to DataFrame\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train_c.csv')\n",
    "train_df = train_df.sample(frac=1)  # shuffle the train data\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "val_df = pd.read_csv(f'{DATA_DIR}/val.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test_no_label.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RrvVCnw53JU"
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QatMlpq6VUCD"
   },
   "source": [
    "#### Prep Data\n",
    "Extract the X,y values and encode the classes into integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wg-B02o5VzBG"
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train_txt = train_df['Class'].values.tolist()\n",
    "le.fit(y_train_txt)\n",
    "labels = le.classes_\n",
    "\n",
    "X_train = train_df.iloc[:,:-2].values.tolist()\n",
    "y_train = le.transform(y_train_txt)\n",
    "\n",
    "X_val = val_df.iloc[:,:-2].values.tolist()\n",
    "y_val_txt = val_df['Class'].values.tolist()\n",
    "y_val = le.transform(y_val_txt)\n",
    "\n",
    "X_test = test_df.iloc[:,:-1].values.tolist()\n",
    "\n",
    "labels = train_df['Class'].unique()\n",
    "\n",
    "list(le.inverse_transform([2, 2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZfOpabjL4kf"
   },
   "source": [
    "# üñºÔ∏è EDA with W&B Tables\n",
    "Log the train and validation datasets to W&B Tables for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dD_7X40vHpQ0"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=PROJECT, job_type='log_dataset')\n",
    "wandb.log({'Datasets/train_ds':train_df})\n",
    "wandb.log({'Datasets/val_ds':val_df})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5ni9t9sfArq"
   },
   "source": [
    "#üëü Train\n",
    "Train a [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) from sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2OZsuwfkVMv"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=PROJECT)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# ‚úçÔ∏è Log your Models parameter config to W&B\n",
    "wandb.config.update(model.get_params())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_val)\n",
    "y_probas = model.predict_proba(X_val)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckURDaqhrhui"
   },
   "source": [
    "‚úçÔ∏è Log your model's Metrics to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b7099kdmHmp"
   },
   "outputs": [],
   "source": [
    "log_metrics(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9FEchN-sXsn"
   },
   "source": [
    "#ü§© Visualize Model Performance in W&B\n",
    "Weights & Biases have charting functions for popular model evaluation charts including confusion matrices, ROC curves, PR curves and more.\n",
    "[Check out wandb charts documentation here $\\rightarrow$](https://docs.wandb.ai/guides/track/log/plots#model-evaluation-charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7wQwS8voSB3"
   },
   "source": [
    "**Confusion Matrix**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0T95FIJmQXM"
   },
   "outputs": [],
   "source": [
    "wandb.log({\"confusion Matrix\" : wandb.plot.confusion_matrix(y_probas, y_val, class_names=labels)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATGYTRKnnQBD"
   },
   "source": [
    "**ROC Curve**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OO8_V_AanPq9"
   },
   "outputs": [],
   "source": [
    "wandb.log({\"ROC Curve\": wandb.plot.roc_curve(y_val, y_probas, labels=labels, title='ROC Curve')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZq5a6RpoapA"
   },
   "source": [
    "**Precision Recall Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31ZEnsbRocJl"
   },
   "outputs": [],
   "source": [
    "wandb.log({\"Precision-Recall\": wandb.plot.pr_curve(y_val, y_probas, labels=labels, title='Precision-Recall')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV66Jrrdolhz"
   },
   "source": [
    "**Feature Importances**\n",
    "\n",
    "Evaluates and plots the importance of each feature for the classification task. Only works with classifiers that have a `feature_importances_` attribute, like trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wv-ED9j_qCZA"
   },
   "outputs": [],
   "source": [
    "feat_names = train_df.columns.values\n",
    "imps = []\n",
    "feats = []\n",
    "for i in indices:\n",
    "  imps.append(importances[i])\n",
    "  feats.append(feat_names[i])\n",
    "\n",
    "fi_data = pd.DataFrame({\"Feature\":feats, \"Importance\":imps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEdaNt9tpqTO"
   },
   "outputs": [],
   "source": [
    "table = wandb.Table(data=fi_data, columns = [\"Feature\", \"Importance\"])\n",
    "wandb.log({\"Feature Importance\" : wandb.plot.bar(table, \"Feature\",\n",
    "                               \"Importance\", title=\"Feature Importance\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqO3vXwAuXUT"
   },
   "source": [
    "#### üèÅ Finish W&B Run\n",
    "When you're finished with your logging for a run make sure to call `wandb.finish()` to avoid logging metrics from your next experiment to the wrong run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIHtZ-QEuVV8"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DesBwID32HJ"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZq_oR8f30yY"
   },
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test = list(le.inverse_transform(y_pred_test))\n",
    "ids = test_df.id.values\n",
    "\n",
    "submission_df = pd.DataFrame({'Id':ids, 'Predicted':y_pred_test})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvVBjQTbpFhV"
   },
   "source": [
    "# üß™ HyperParameter Sweep\n",
    "\n",
    "Weights and Biases also enables you to do hyperparameter sweeps, either with our own [Sweeps functionality](https://docs.wandb.ai/guides/sweeps/python-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ixD36XLozvk"
   },
   "source": [
    "#### Sweep Train Function\n",
    "A W&B Sweep needs to passed in a config and a training function to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6TwD4gSsrwo"
   },
   "outputs": [],
   "source": [
    "def train():     \n",
    "    with wandb.init() as _:\n",
    "      \n",
    "      model = RandomForestClassifier(\n",
    "          \n",
    "          n_estimators=wandb.config['n_estimators'],   # n_estimators parameter will now be set by W&B\n",
    "          max_depth=wandb.config['max_depth']     # max_depth parameter will now be set by W&B\n",
    "          \n",
    "          # [Optional] add additional model parameters here\n",
    "          \n",
    "          )\n",
    "      \n",
    "      # ‚úçÔ∏è Log your Models parameter config to W&B\n",
    "      wandb.config['model_type'] = 'random_forest'\n",
    "      wandb.config.update(model.get_params())\n",
    "\n",
    "      model.fit(X_train, y_train)\n",
    "\n",
    "      y_pred_train = model.predict(X_train)\n",
    "      y_pred = model.predict(X_val)\n",
    "      y_probas = model.predict_proba(X_val)\n",
    "        \n",
    "      # Log validation summary metrics to W&B\n",
    "      wandb.summary[\"validation/accuracy\"] = accuracy_score(y_val, y_pred)\n",
    "      wandb.summary[\"validation/precision\"] = precision_score(y_val, y_pred, average=\"weighted\")\n",
    "      wandb.summary[\"validation/recall\"] = recall_score(y_val, y_pred, average=\"weighted\")\n",
    "      wandb.summary[\"validation/f1_score\"] = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "    \n",
    "      # Make test set predictions and save as csv  \n",
    "      y_pred_test = model.predict(X_test)\n",
    "      y_pred_test = list(le.inverse_transform(y_pred_test))\n",
    "\n",
    "      submission_df = pd.DataFrame({'Id':test_df.id.values, 'Predicted':y_pred_test})\n",
    "      submission_df.to_csv('submission.csv', index=False)\n",
    "        \n",
    "      wandb.log_artifact('submission.csv', name=f'{wandb.run.id}_submission', type='submission')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHaoTUwkqxNl",
    "tags": []
   },
   "source": [
    "üí° **Tip**\n",
    "\n",
    "The `train` function above uses Sci-Kit Learn's [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) but you can also modify the code to user other models such as `DecisionTreeClassifier` or `AdaBoostClassifier` or other boosting models such as [`XGBoost`](https://xgboost.readthedocs.io/en/latest/get_started.html). \n",
    "\n",
    "Note that you'll likely have to chanage the argument names in the `sweep_config` when using these models in a sweep.\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model = AdaBoostClassifier()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Sweep Config\n",
    "Define the name of your sweep, how you'd like to sweep and what parameters to sweep over. See the [Sweep Configuration Docs](https://docs.wandb.ai/guides/sweeps/configuration) here for more advanced functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmPoRCjOsrzQ"
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"name\" : \"beans_sweep\",\n",
    "  \"method\" : \"random\",\n",
    "  \"parameters\" : {\n",
    "    \"n_estimators\" :{\n",
    "      \"min\": 10,\n",
    "      \"max\": 400\n",
    "    },\n",
    "    \"max_depth\" :{\n",
    "      \"min\": 2,\n",
    "      \"max\": 100\n",
    "    },\n",
    "\n",
    "    # [Optional] add additional parameters here\n",
    "\n",
    "  }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlWEExyQosPw"
   },
   "source": [
    "üí° **Tip**\n",
    "\n",
    "The above `sweeps_config` is very simple, consider sweeping over additional parameters - don't forget to modify your `train` function to pass these additional parameters to your model\n",
    "\n",
    "####¬†Run Sweep\n",
    "Now we define the number of experiments we'd like to run using `N_RUNS`, pass the sweep_id and the training function and then start the sweep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvEEKlALsrsN"
   },
   "outputs": [],
   "source": [
    "N_RUNS = 50 # number of runs to execute\n",
    "wandb.agent(sweep_id, project=PROJECT, function=train, count=N_RUNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nf_hHZrhDe7D"
   },
   "source": [
    "# Get Submission Files From a Sweeps Run\n",
    "\n",
    "You can download the test set predictions from each Sweeps run via Artifacts. Or you can find which parameters resulted in the best trained model, and replicate that training using the training and prediction code in the first part of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0vTCNSsDe7E"
   },
   "outputs": [],
   "source": [
    "# Find the run_id of the best performing Sweeps run, it can be found in the URL in the W&B UI\n",
    "RUN_ID = 'abc123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tb4sV3ZDe7E"
   },
   "outputs": [],
   "source": [
    "# Download the submission file from artifacts\n",
    "wandb.init(project=PROJECT, job_type='download_submission')\n",
    "artifact = wandb.use_artifact(f'{wandb.run.entity}/{PROJECT}/{RUN_ID}_submission:latest', type='submission')\n",
    "artifact_dir = artifact.download('my_submissions')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A4CVu1L1zN0"
   },
   "source": [
    "# ü™Ñ More from W&B\n",
    "#### üé® Example Gallery\n",
    "\n",
    "See examples of projects tracked and visualized with W&B in our gallery, [Fully Connected‚Üí](https://app.wandb.ai/gallery)\n",
    "\n",
    "#### üèôÔ∏è Community\n",
    "\n",
    "Join a community of ML practitioners in our\n",
    "[Discourse forum‚Üí](http://wandb.me/and-you)\n",
    "\n",
    "#### üìè Best Practices\n",
    "\n",
    "1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n",
    "2. **Groups**: For multiple processes or cross validation folds, log each process as a run and group them together. `wandb.init(group='experiment-1')`\n",
    "3. **Tags**: Add tags to track your current baseline or production model.\n",
    "4. **Notes**: Type notes in the table to track the changes between runs.\n",
    "5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.\n",
    "\n",
    "#### ü§ì Advanced Setup\n",
    "\n",
    "1. [Environment variables](https://docs.wandb.com/library/environment-variables): Set API keys in environment variables so you can run training on a managed cluster.\n",
    "2. [Offline mode](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): Use `dryrun` mode to train offline and sync results later.\n",
    "3. [On-prem](https://docs.wandb.com/self-hosted): Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams.\n",
    "4. [Sweeps](http://wandb.me/sweeps-colab): Set up hyperparameter search quickly with our lightweight tool for tuning.\n",
    "5. [Artifacts](http://wandb.me/artifacts-colab): Track and version models and datasets in a streamlined way that automatically picks up your pipeline steps as you train models.\n",
    "6. [Tables](http://wandb.me/dsviz-nature-colab): Log, query, and analyze tabular data. Understand your datasets, visualize model predictions, and share insights in a central dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYOijKVbDe7F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "W&B_and_University of Stavanger_Bean_Leaf_Classification.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
